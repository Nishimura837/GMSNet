{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training用のコード\n",
    "### 損失関数の求め方、必要な値が正しく得られているかなど検証してみる\n",
    "2024/5/17 時点での完成版\n",
    "\n",
    "コードをすべて問題なく実行できるようにはなった。\n",
    "最適化されたメッシュを取り出してくることもでき、しっかり更新できている。\n",
    "しかし、損失の値が全く減少せず、むしろ増加していてほぼもともとのメッシュのままの出力が最適化メッシュとして出てきてしまう。\n",
    "ゆえに、最適化できているとは到底言えない。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainを実行するためのコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・各epoch後のメッシュの表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・学習率を0.1倍から0.95倍にしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・シフト切り捨ては推論時飲み使用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・バッチサイズを変えてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・モデルの出力を0.5倍するなどして移動量を小さくする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・学習時はMetricLossで、最適メッシュを選定するときは違う関数なのはなんで"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シフト切り捨てをやめてみる\n",
    "# トレーニング完了後のトレーニングメッシュを表示してみる\n",
    "# 各epochの終了時にメッシュを可視化してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphNorm, LayerNorm\n",
    "import torch_geometric.transforms as T\n",
    "from torch.nn import Linear, InstanceNorm2d, InstanceNorm1d, Conv1d, ReLU, Tanh\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.transforms import FaceToEdge\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from itertools import combinations\n",
    "import vtk\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "# 計算を軽くするためのライブラリ\n",
    "from torch.cuda import empty_cache\n",
    "import gc               # メモリリークを防ぐ\n",
    "\n",
    "from torch import nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, num_files):\n",
    "        None\n",
    "\n",
    "class Mesh(Dataset):\n",
    "    def __init__(self):\n",
    "        self.coordinates = None\n",
    "        self.faces = None\n",
    "\n",
    "class Polygon(Dataset):\n",
    "    def __init__(self, num_node, num_face):\n",
    "        self.parent_meshID = None\n",
    "        self.coordinates = torch.zeros(num_node, 2)\n",
    "        self.faces = torch.zeros(num_face, 3)\n",
    "        self.edges = None\n",
    "        self.d = None\n",
    "        self.Cx = None\n",
    "        self.Cy = None\n",
    "        self.x_min = None\n",
    "        self.y_min = None\n",
    "\n",
    "class PolygonID(Dataset):\n",
    "    def __init__(self, nodeID):\n",
    "        self.nodeID = nodeID\n",
    "        # self.parent_meshID = None\n",
    "\n",
    "class Polygon_data(Dataset):\n",
    "    def __init__(self, polygonID, meshID, nodeID):\n",
    "        self.polygonID = polygonID\n",
    "        self.meshID = meshID\n",
    "        self.nodeID = nodeID\n",
    "\n",
    "class Minibatch(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.edge_index = None\n",
    "        self.batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh_polygonID_data(vtk_file_path, polygonID_list, poly_count, polygon_dict, mesh_index):\n",
    "    reader = vtk.vtkDataSetReader()\n",
    "    reader.SetFileName(vtk_file_path)\n",
    "    reader.Update()\n",
    "\n",
    "    data = reader.GetOutput()\n",
    "    \n",
    "    mesh = Mesh()\n",
    "    # 座標情報を取得\n",
    "    points = data.GetPoints()\n",
    "    num_points = points.GetNumberOfPoints()\n",
    "    coordinates = torch.zeros(num_points, 3)\n",
    "    for i in range(num_points):\n",
    "        coordinates[i] = torch.tensor(points.GetPoint(i))\n",
    "\n",
    "    mesh.coordinates = coordinates[:, :2]                        # mesh.coordinates を定義\n",
    "\n",
    "    # 面情報を取得\n",
    "    polys = data.GetPolys()\n",
    "    num_polys = polys.GetNumberOfCells()\n",
    "    mesh.faces = torch.zeros(num_polys, 3, dtype=int)           # mesh.faces を定義\n",
    "\n",
    "    # 各三角形の情報を取得\n",
    "    polys.InitTraversal()\n",
    "    for i in range(num_polys):\n",
    "        cell = vtk.vtkIdList()\n",
    "        if polys.GetNextCell(cell) == 0:\n",
    "            break\n",
    "        mesh.faces[i] = torch.tensor([cell.GetId(0), cell.GetId(1), cell.GetId(2)])\n",
    "        \n",
    "# ------------ mesh のデータを取得完了 -------------------------\n",
    "\n",
    "\n",
    "    # 各セルの各辺の隣接セル数を調べる\n",
    "    edge_neighbors = {}\n",
    "    num_cells = data.GetNumberOfCells()\n",
    "    for cell_index in range(num_cells):\n",
    "        cell = data.GetCell(cell_index)\n",
    "        num_edges = cell.GetNumberOfEdges()\n",
    "\n",
    "        for edge_index in range(num_edges):\n",
    "            edge = cell.GetEdge(edge_index)\n",
    "            edge_points = edge.GetPointIds()\n",
    "\n",
    "            # 辺を構成する点のインデックスを取得\n",
    "            point1_id = edge_points.GetId(0)\n",
    "            point2_id = edge_points.GetId(1)\n",
    "\n",
    "            # 辺を構成する点のインデックスを昇順にソート\n",
    "            edge_key = (min(point1_id, point2_id), max(point1_id, point2_id))\n",
    "\n",
    "            # 辺の隣接セル数をカウント\n",
    "            if edge_key in edge_neighbors:\n",
    "                edge_neighbors[edge_key] += 1\n",
    "            else:\n",
    "                edge_neighbors[edge_key] = 1 \n",
    "\n",
    "    boundary_edges = []\n",
    "    # 境界上の辺を特定\n",
    "    for edge_key, num_neighbors in edge_neighbors.items():\n",
    "        if num_neighbors == 1:\n",
    "            boundary_edges.append(edge_key)\n",
    "\n",
    "    # 境界上の辺を構成する頂点の番号を取得\n",
    "    boundary_points = set()     # 集合を表すデータ型、順番を持たず、重複した要素は取り除かれる\n",
    "# ---------------- 自由点かどうかの判定完了 ------------------------\n",
    "    \n",
    "\n",
    "    for edge_key in boundary_edges:\n",
    "        boundary_points.add(edge_key[0])\n",
    "        boundary_points.add(edge_key[1])\n",
    "    \n",
    "    \n",
    "    for pointId in range(num_points):       # pointId:自由点の頂点番号\n",
    "        if pointId in boundary_points:\n",
    "            continue\n",
    "        else:\n",
    "            poly_count += 1\n",
    "            # print(\"pointId:\", pointId)\n",
    "        mask = (mesh.faces == pointId)\n",
    "        if mask.any():\n",
    "            count = torch.sum(mask).item()\n",
    "        num_node = count + 1\n",
    "        num_face = count\n",
    "        polygon_number = poly_count - 1 \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        polygon_i = f\"polygon_{polygon_number}\"\n",
    "        # print(polygon_i)\n",
    "        polygon_i = Polygon(num_node, num_face)\n",
    "        \n",
    "        element_to_check = pointId\n",
    "        polygon_i.face = mesh.faces[(mesh.faces == element_to_check).any(dim=1)]\n",
    "        # print(polygon_i.face)\n",
    "\n",
    "        polygon_i.nodeId = set()\n",
    "        for i in range(len(polygon_i.face)):\n",
    "            polygon_i.nodeId.add(polygon_i.face[i, 0].item())\n",
    "            polygon_i.nodeId.add(polygon_i.face[i, 1].item())\n",
    "            polygon_i.nodeId.add(polygon_i.face[i, 2].item())\n",
    "        sorted_nodeId = sorted(polygon_i.nodeId)\n",
    "        polygon_i.nodeID = torch.tensor(list(sorted_nodeId))\n",
    "        \n",
    "        point_id_index = (polygon_i.nodeID == pointId).nonzero().item()\n",
    "\n",
    "        value_to_move = polygon_i.nodeID[point_id_index]\n",
    "        polygon_i.nodeID = torch.cat((value_to_move.unsqueeze(0), polygon_i.nodeID[polygon_i.nodeID != pointId]))\n",
    "        # print(polygon_i.nodeID)\n",
    "        setattr(polygon_i, \"parent_meshID\", mesh)\n",
    "        polygonID_list.append(f\"polygon_{polygon_number}\")\n",
    "\n",
    "        keyword = f\"polygon_{polygon_number}\"\n",
    "        valiables = (f\"mesh_{mesh_index}\", polygon_i.nodeID)\n",
    "        polygon_dict[keyword] = valiables\n",
    "\n",
    "    # --------- polygon.nodeID の取得完了 -------------\n",
    "    return mesh, polygonID_list, poly_count, polygon_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh_polygon_dataset(vtk_files):\n",
    "    num_vtk_files = len(vtk_files)\n",
    "    polygonID_list = []\n",
    "    mesh_data_list = []\n",
    "    poly_count = 0\n",
    "    polygon_dict = {}\n",
    "    # ファイルに順にアクセスする\n",
    "    for i in range(num_vtk_files):\n",
    "        # print(\"File Name:\", vtk_files[i])\n",
    "        mesh, polygonID_list, poly_count, polygon_dict = create_mesh_polygonID_data(vtk_files[i], polygonID_list, poly_count, polygon_dict, i)\n",
    "        mesh_data_list.append(mesh)\n",
    "    return mesh_data_list, polygonID_list, polygon_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下、i はpolygon番号で座標と面情報を取得することができる\n",
    "face_to_edge = T.FaceToEdge(remove_faces=False)\n",
    "def data_getter(polygonID, num_mesh_data_list, mesh_data_list, polygon_data_list):\n",
    "    \n",
    "    polygon_meshID = int(polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "    mesh = mesh_data_list[polygon_meshID]\n",
    "    \n",
    "    num_node = len(polygon_data_list[polygonID].nodeID)\n",
    "    num_face = num_node - 1 \n",
    "    polygon_i = Polygon(num_node, num_face)\n",
    "\n",
    "    # print(polygon_data_list[polygonID].nodeID)      # polygon に属する頂点の番号\n",
    "\n",
    "    polygon_i.coordinates = mesh.coordinates[polygon_data_list[polygonID].nodeID]     # polygonの座標\n",
    "    # print(polygon_i.coordinates)\n",
    "\n",
    "    # print(polygon_i.faces)\n",
    "\n",
    "    # polygon_i.faces を取得するコード\n",
    "    \n",
    "    element_to_check = polygon_data_list[polygonID].nodeID[0]\n",
    "    polygon_i.face = mesh.faces[(mesh.faces == element_to_check).any(dim=1)]\n",
    "\n",
    "    indices = torch.nonzero(torch.isin(polygon_i.face, polygon_data_list[polygonID].nodeID))\n",
    "    for idx in range(indices.size(0)):\n",
    "        row_idx, col_idx = indices[idx]\n",
    "        value_to_replace = polygon_i.face[row_idx, col_idx]\n",
    "        polygon_i.face[row_idx, col_idx] = (polygon_data_list[polygonID].nodeID == value_to_replace).nonzero().item()\n",
    "    polygon_i.faces = polygon_i.face.long()\n",
    "\n",
    "    # 各行の三角形からエッジを抽出してedge_indexを構築\n",
    "    edges = torch.cat([ polygon_i.faces[:, [0, 1]],\n",
    "                        polygon_i.faces[:, [1, 2]],\n",
    "                        polygon_i.faces[:, [2, 0]]], dim=0)\n",
    "\n",
    "    # エッジのインデックスをソートして重複を削除\n",
    "    edge_index = torch.sort(edges, dim=1).values\n",
    "    edge_index = torch.tensor(sorted(edge_index.numpy().tolist())).unique(dim=0)\n",
    "    polygon_i.edge_index = torch.transpose(edge_index, 0, 1)\n",
    "    return polygon_i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メッシュをプロットする関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(mesh, title):\n",
    "\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect=\"equal\")\n",
    "\n",
    "    # 描画するメッシュの頂点をプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'bo')  # 頂点を青色の点でプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'k-')  # 辺を黒色の線でプロット\n",
    "\n",
    "    # 各三角形をプロット\n",
    "    for face in faces:\n",
    "        v0, v1, v2 = vertices[face]\n",
    "        v0_np = v0.detach().numpy()\n",
    "        v1_np = v1.detach().numpy()\n",
    "        v2_np = v2.detach().numpy()\n",
    "        ax.plot([v0_np[0], v1_np[0], v2_np[0], v0_np[0]], [v0_np[1], v1_np[1], v2_np[1], v0_np[1]], 'b-')  # 三角形を赤色の線でプロット\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.001)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.001)\n",
    "\n",
    "    # plt.xlim(-300, 150)\n",
    "    # plt.ylim(-200, 1400)\n",
    "    # plt.savefig(f\"/mnt/{title}.png\", format=\"png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mesh(mesh, title):\n",
    "\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect=\"equal\")\n",
    "\n",
    "    # 描画するメッシュの頂点をプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'bo')  # 頂点を青色の点でプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'k-')  # 辺を黒色の線でプロット\n",
    "\n",
    "    # 各三角形をプロット\n",
    "    for face in faces:\n",
    "        v0, v1, v2 = vertices[face]\n",
    "        v0_np = v0.detach().numpy()\n",
    "        v1_np = v1.detach().numpy()\n",
    "        v2_np = v2.detach().numpy()\n",
    "        ax.plot([v0_np[0], v1_np[0], v2_np[0], v0_np[0]], [v0_np[1], v1_np[1], v2_np[1], v0_np[1]], 'b-')  # 三角形を赤色の線でプロット\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.001)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.001)\n",
    "\n",
    "    # plt.xlim(-300, 150)\n",
    "    # plt.ylim(-200, 1400)\n",
    "    plt.savefig(f\"/mnt/Saved_mesh2/{title}.png\", format=\"png\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meshデータからvtkファイルを出力する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtk_output(mesh, title):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    num_vertices = len(vertices)\n",
    "    num_faces = len(faces)\n",
    "\n",
    "\n",
    "    # vertices を３次元に戻す\n",
    "    z_column = torch.zeros(vertices.shape[0], 1)\n",
    "    vertices = torch.cat((vertices, z_column), dim=1)\n",
    "\n",
    "    with open(f\"/mnt/optimized_data/{title}.vtk\", \"w\") as f:\n",
    "        f.write(\"# vtk DataFile Version 2.0\\n\")\n",
    "        f.write(\"FOR TEST\\n\")\n",
    "        f.write(\"ASCII\\n\")\n",
    "        f.write(\"DATASET POLYDATA\\n\")\n",
    "\n",
    "        f.write(\"POINTS {} float\\n\".format(num_vertices))\n",
    "        for vertex in vertices:\n",
    "            f.write(\"{:.15f} {:.15f} {:.15f}\\n\".format(*vertex))\n",
    "\n",
    "        f.write(\"\\nPOLYGONS {} {}\\n\".format(num_faces, num_faces * 4))\n",
    "        for face in faces:\n",
    "            f.write(\"3 \")\n",
    "            f.write(\" \".join(str(idx.item()) for idx in face))\n",
    "            f.write(\"\\n\")\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(polygon):\n",
    "    vertices = polygon.coordinates\n",
    "    normalized_vertices = vertices.clone()\n",
    "    centered_vertices = vertices.clone()\n",
    "    # print(vertices)\n",
    "\n",
    "    max_x = torch.max(vertices[:,0])\n",
    "    min_x = torch.min(vertices[:,0])\n",
    "    max_y = torch.max(vertices[:,1])\n",
    "    min_y = torch.min(vertices[:,1])\n",
    "\n",
    "    polygon.d = torch.max(max_x - min_x, max_y - min_y)\n",
    "    polygon.x_min = min_x\n",
    "    polygon.y_min = min_y\n",
    "\n",
    "    normalized_vertices = (vertices - torch.tensor([polygon.x_min, polygon.y_min])) / polygon.d\n",
    "\n",
    "    \n",
    "    polygon.Cx = normalized_vertices[0,0].item()\n",
    "    polygon.Cy = normalized_vertices[0,1].item()\n",
    "\n",
    "    centered_vertices = normalized_vertices - torch.tensor([polygon.Cx, polygon.Cy])\n",
    "    polygon.coordinates = centered_vertices\n",
    "    \n",
    "    # print(\"Normalized polygon:\", vertices)\n",
    "\n",
    "    return polygon\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(polygon):\n",
    "    vertices = polygon.coordinates\n",
    "    shifted_vertices = vertices.clone()\n",
    "    denormalized_vertices = vertices.clone()\n",
    "    \n",
    "    shifted_vertices = vertices + torch.tensor([polygon.Cx, polygon.Cy])\n",
    "        \n",
    "\n",
    "    denormalized_vertices = polygon.d * shifted_vertices + torch.tensor([polygon.x_min, polygon.y_min])\n",
    "    polygon.coordinates = denormalized_vertices\n",
    "    return polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログをファイルに保存するようにしている\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='metric_loss.log',\n",
    "    level=logging.DEBUG, \n",
    "    format='%(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MetricLoss:\n",
    "    def select_vertices(self, vertices, face):\n",
    "        v0 = vertices[face[0]].clone()\n",
    "        v1 = vertices[face[1]].clone()\n",
    "        v2 = vertices[face[2]].clone()\n",
    "        return v0, v1, v2 \n",
    "\n",
    "    def edge_length(self, v0, v1, v2):\n",
    "        l1 = torch.sqrt((v0[0] - v1[0])**2 + (v0[1] - v1[1])**2)\n",
    "        l2 = torch.sqrt((v1[0] - v2[0])**2 + (v1[1] - v2[1])**2)\n",
    "        l3 = torch.sqrt((v2[0] - v0[0])**2 + (v2[1] - v0[1])**2)\n",
    "\n",
    "        return l1, l2, l3\n",
    "\n",
    "    def face_area(self, polygon, l1, l2, l3):   # エラーのときはポリゴンを表示する\n",
    "        \n",
    "        s = 0.5*(l1 + l2 + l3)\n",
    "        #print(round(l1.item(),5), round(l2.item(),5), round(l3.item(),5), round(s.item(),5), round((s*(s-l1)*(s-l2)*(s-l3)).item(),5))\n",
    "        temp = s*(s-l1)*(s-l2)*(s-l3)\n",
    "        #temp.register_hook(print_grad)\n",
    "        logger.info(\"    s, in_sqrt: {}, {}\".format(s.item(), temp.item()))\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            face_area = torch.sqrt(temp)\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            SimplePolygonGenerator.polygon_visualizer(polygon)\n",
    "            \n",
    "            print(\"An error occurred\")\n",
    "            print(\"Value of temp:\", temp)\n",
    "            print(l1.item(), l2.item(), l3.item())\n",
    "            \n",
    "            raise\n",
    "        \n",
    "        #face_area = torch.sqrt(temp)\n",
    "        #face_area.register_hook(print_grad)\n",
    "        return face_area\n",
    "\n",
    "    def compute_loss(self, polygon, vertices, face, dx):\n",
    "        v0, v1, v2 = self.select_vertices(vertices, face)\n",
    "        if dx is not None:          # わからんけどどれか原点にあるやつが自由点だからそれを移動させよう\n",
    "            if face[0]==0:\n",
    "                v0 = v0 + dx\n",
    "            elif face[1]==0:\n",
    "                v1 += v1 + dx\n",
    "            elif face[2]==0:\n",
    "                v2 += v2 + dx\n",
    "        #print(v0, v1, v2)\n",
    "        logger.info(\"    v0: ({}, {})\".format(v0[0].item(), v0[1].item()))\n",
    "        logger.info(\"    v1: ({}, {})\".format(v1[0].item(), v1[1].item()))\n",
    "        logger.info(\"    v2: ({}, {})\".format(v2[0].item(), v2[1].item()))\n",
    "        l1, l2, l3 = self.edge_length(v0, v1, v2)\n",
    "        logger.info(\"    l1, l2, l3:  {}, {}, {}\".format(l1.item(), l2.item(), l3.item()))\n",
    "        s = self.face_area(polygon, l1, l2, l3)\n",
    "        #print(s.item(), l1.item(), l2.item(), l3.item())\n",
    "\n",
    "        #q = (l1**2 + l2**2 + l3**2)/(4.0*torch.sqrt(torch.tensor(3.))*s+1.0)\n",
    "        \n",
    "        #loss = 1 - 1/q\n",
    "        #print(q.item(), loss.item())\n",
    "        \n",
    "        #q = q.clone().detach().requires_grad_(True)\n",
    "        loss = 1-(4.0*torch.sqrt(torch.tensor(3.))*s)/(l1**2 + l2**2 + l3**2)\n",
    "        logger.info(\"    area, loss: {}, {}\".format(s.item(), loss.item()))\n",
    "        logger.info(\"\")\n",
    "        #print(loss.item())\n",
    "        #print(\"\")\n",
    "        \n",
    "        #loss.register_hook(print_grad)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def __call__(self, polygon, dx=None):\n",
    "        vertices = polygon.coordinates\n",
    "        faces = polygon.faces\n",
    "        loss = 0 \n",
    "        #print(vertices)\n",
    "        #print(dx)\n",
    "        for face in faces:\n",
    "            loss = loss + self.compute_loss(polygon, vertices, face, dx)\n",
    "        \n",
    "        metric_loss = loss/(len(polygon.coordinates[:,0])-1) #.clone().detach().requires_grad_(True))\n",
    "        return metric_loss\n",
    "    \n",
    "    \n",
    "def print_grad(grad):\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meshデータからq_hatを求める関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_hat(mesh):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    r_list = []\n",
    "    alpha_list = []\n",
    "    beta_list = []\n",
    "\n",
    "    for face in faces:\n",
    "        # a(最小角)と b(最大角)を求める\n",
    "\n",
    "        angles = []\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "\n",
    "        # 余弦定理から各角度の余弦値を計算\n",
    "        cos_alpha = (l2**2 + l3**2 - l1**2) / (2*l2*l3)\n",
    "        cos_beta = (l1**2 + l3**2 - l2**2) / (2*l1*l3)\n",
    "        cos_gamma = (l1**2 + l2**2 - l3**2) / (2*l1*l2)\n",
    "        # 余弦値から角度を計算して個度法に変換\n",
    "        alpha = torch.acos(cos_alpha) * 180 / np.pi\n",
    "        beta = torch.acos(cos_beta) * 180 / np.pi\n",
    "        gamma = torch.acos(cos_gamma) * 180 / np.pi\n",
    "\n",
    "        angles.append(alpha)\n",
    "        angles.append(beta)\n",
    "        angles.append(gamma)\n",
    "\n",
    "        min_angle = min(angles)\n",
    "        max_angle = max(angles)\n",
    "\n",
    "        alpha_list.append(min_angle)\n",
    "        beta_list.append(max_angle)\n",
    "\n",
    "    # 1/q = r を求める\n",
    "    for i in range(len(test_polygonID_list)):\n",
    "        polygonID = i \n",
    "        polygon = data_getter(polygonID, 0, test_mesh_data_lists, test_polygon_data_list)\n",
    "\n",
    "        r = 1 - m_loss(polygon) \n",
    "        r_list.append(r)\n",
    "\n",
    "    a_mean = sum(alpha_list) / len(alpha_list)\n",
    "    a_min = min(alpha_list)\n",
    "    b_mean = sum(beta_list) / len(beta_list)\n",
    "    b_max = max(beta_list)\n",
    "    r_mean = sum(r_list) / len(r_list)\n",
    "    r_min = min(r_list)\n",
    "\n",
    "    q_hat = (((a_mean + a_min + 120 - b_max - b_mean)/60) + r_mean + r_min) / 6\n",
    "\n",
    "    return q_hat\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_qhat(mesh):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    r_list = []\n",
    "    alpha_list = []\n",
    "    beta_list = []\n",
    "\n",
    "    for face in faces:\n",
    "        # a(最小角)と b(最大角)を求める\n",
    "\n",
    "        angles = []\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "\n",
    "        # 余弦定理から各角度の余弦値を計算\n",
    "        cos_alpha = (l2**2 + l3**2 - l1**2) / (2*l2*l3)\n",
    "        cos_beta = (l1**2 + l3**2 - l2**2) / (2*l1*l3)\n",
    "        cos_gamma = (l1**2 + l2**2 - l3**2) / (2*l1*l2)\n",
    "        # 余弦値から角度を計算して個度法に変換\n",
    "        alpha = torch.acos(cos_alpha) * 180 / np.pi\n",
    "        beta = torch.acos(cos_beta) * 180 / np.pi\n",
    "        gamma = torch.acos(cos_gamma) * 180 / np.pi\n",
    "\n",
    "        angles.append(alpha)\n",
    "        angles.append(beta)\n",
    "        angles.append(gamma)\n",
    "\n",
    "        min_angle = min(angles)\n",
    "        max_angle = max(angles)\n",
    "\n",
    "        alpha_list.append(min_angle)\n",
    "        beta_list.append(max_angle)\n",
    "\n",
    "        # 1/q = r を求める\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "        s = 0.5*(l1 + l2 + l3)\n",
    "        temp = s*(s-l1)*(s-l2)*(s-l3)\n",
    "        loss = 1-(4.0*torch.sqrt(torch.tensor(3.))*s)/(l1**2 + l2**2 + l3**2)\n",
    "        r_list.append(1./loss)\n",
    "\n",
    "\n",
    "\n",
    "    a_mean = sum(alpha_list) / len(alpha_list)\n",
    "    a_min = min(alpha_list)\n",
    "    b_mean = sum(beta_list) / len(beta_list)\n",
    "    b_max = max(beta_list)\n",
    "    r_mean = sum(r_list) / len(r_list)\n",
    "    r_min = min(r_list)\n",
    "\n",
    "    q_hat = (((a_mean + a_min + 120 - b_max - b_mean)/60) + r_mean + r_min) / 6\n",
    "\n",
    "    return q_hat\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スターポリゴンの中から外側に自由点が移動したときに自由点の移動量を半分にしてもう一度外に行っていないか検証する\n",
    "自由点が外に行かないことを確認したあとのスターポリゴンを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(polygon, polygonID):\n",
    "    # print(\"polygonID:\", polygonID)\n",
    "    vertices = polygon.coordinates\n",
    "    \n",
    "    edge_index = polygon.edge_index\n",
    "    \n",
    "    return_value = True\n",
    "    while return_value == True:   \n",
    "\n",
    "        for i in range(1, len(vertices[:,0])):\n",
    "            point1 = torch.tensor([0.0, 0.0])\n",
    "            point2 = vertices[0]\n",
    "            point3 = vertices[i]\n",
    "            \n",
    "            pos_i = torch.where(edge_index[0] == i)\n",
    "            pos_i = pos_i[0]\n",
    "            # print(\"edge_index[0]\", edge_index[0])\n",
    "            \n",
    "            for j in range(len(pos_i)):\n",
    "                if edge_index[1, pos_i[j]] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    val_pos_i = edge_index[1, pos_i[j]]\n",
    "                    break\n",
    "\n",
    "            point4 = vertices[val_pos_i]\n",
    "\n",
    "\n",
    "            a1 = 0\n",
    "            b1 = 0\n",
    "            a2 = 0\n",
    "            b2 = 0\n",
    "            check1 = 0\n",
    "            check2 = 0\n",
    "            check3 = 0\n",
    "            check4 = 0\n",
    "            x1 = point1[0]\n",
    "            y1 = point1[1]\n",
    "            x2 = point2[0]\n",
    "            y2 = point2[1]\n",
    "            x3 = point3[0]\n",
    "            y3 = point3[1]\n",
    "            x4 = point4[0]\n",
    "            y4 = point4[1]\n",
    "            a1 = (y1 - y2)/(x1-x2)\n",
    "            b1 = y1 - (a1*x1)\n",
    "            a2 = (y3 - y4)/(x3-x4)\n",
    "            b2 = y3 - (a2*x3)\n",
    "            check1 = (a1*x3) - y3 + b1 \n",
    "            check2 = (a1*x4) - y4 + b1    # point1,2を通る直線に対してpoint3,4を結ぶ線分が交差しているか\n",
    "            check3 = (a2*x1) - y1 + b2\n",
    "            check4 = (a2*x2) - y2 + b2    # point3,4を通る直線に対してpoint1,2を結ぶ線分が交差しているか\n",
    "            # print(\"1:\",check1,\"2:\",check2,\"3:\",check3,\"4:\",check4)\n",
    "            del a1, a2, b1, b2, x1, x2, x3, x4, y1, y2, y3, y4 \n",
    "\n",
    "            if (check1*check2) <= 0 and (check3*check4) <= 0 :\n",
    "                return_value = True\n",
    "                # print(\"Out_of_StarPolygon\")\n",
    "                vertices[0] = 0.5*vertices[0]\n",
    "                polygon.coordinates[0] = vertices[0]\n",
    "                break\n",
    "            else:\n",
    "                return_value = False\n",
    "                continue       \n",
    "            \n",
    "        \n",
    "    # plot_mesh(polygon, \"polygon_checked\")\n",
    "               \n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 隠れ層のノード数は何にするか未定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMSNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim, hidden_channnels):   # モデルの初期化\n",
    "        \n",
    "        super(GMSNet, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.shared_mlp = Conv1d(input_dim, feature_dim, kernel_size=1) # 1次元の畳み込み層（MLP)入力次元を特徴量次元に変換\n",
    "        self.GNorm = GraphNorm(feature_dim, feature_dim)                # グラフノーマライゼーション層、特徴量の正規化============================================================-\n",
    "        self.conv = GCNConv(feature_dim, feature_dim)                   # グラフ構造を考慮した特徴量抽出を行う\n",
    "        self.fc1 = Linear(feature_dim, hidden_channnels)                # 全結合層\n",
    "        #self.ISNorm = InstanceNorm1d(hidden_channnels, affine=True)    \n",
    "        self.fc2 = Linear(hidden_channnels, input_dim)                  # 全結合層\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self._init_weights)                                  # 重みの初期化\n",
    "\n",
    "    def _init_weights(self, m):     # 線形層と畳み込み層の重みをKaiming正規化で初期化し、バイアスをゼロで初期化する\n",
    "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):       \n",
    "        \n",
    "        # feature\n",
    "        x = torch.permute(x, (0, 2, 1))     # polygon.coordinatesを横向きのtensorに変えている\n",
    "        x = self.shared_mlp(x)\n",
    "        feature = self.relu(x)\n",
    "        feature = torch.permute(feature, (0, 2, 1))\n",
    "        \n",
    "        # MLP\n",
    "        target_feature = feature.mean(dim=1)\n",
    "        mlp_midlayer = self.fc1(target_feature)\n",
    "        x = self.relu(mlp_midlayer)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)        #出力の値を制限している\n",
    "        \n",
    "        x = 0.1*x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_mesh: 4\n"
     ]
    }
   ],
   "source": [
    "# フォルダ内のすべてのvtkファイルにアクセスする\n",
    "# train_vtk_files = glob.glob(\"/mnt/new_train_data/*.vtk\")\n",
    "train_vtk_files = glob.glob(\"/mnt/new_train_data/*.vtk\")\n",
    "# eval_vtk_files = glob.glob(\"/mnt/star_re.vtk\")\n",
    "# test_vtk_files = glob.glob(\"/mnt/training_mesh_0.vtk\")\n",
    "\n",
    "num_train_mesh = len(train_vtk_files)\n",
    "# num_test_mesh = len(test_vtk_files)\n",
    "print(\"num_train_mesh:\", num_train_mesh)\n",
    "train_mesh_data_list, train_polygonID_list, train_polygon_dict = create_mesh_polygon_dataset(train_vtk_files)\n",
    "# eval_mesh_data_list, eval_polygonID_list, eval_polygon_dict = create_mesh_polygon_dataset(eval_vtk_files)\n",
    "# test_mesh_data_list, test_polygonID_list, test_polygon_dict = create_mesh_polygon_dataset(test_vtk_files)\n",
    "\n",
    "\n",
    "    \n",
    "# ポリゴンデータを格納するリストを作成\n",
    "train_polygon_data_list = []\n",
    "# eval_polygon_data_list = []\n",
    "# test_polygon_data_list = []\n",
    "\n",
    "for i in range(len(train_polygonID_list)):\n",
    "    polygonID = f\"polygon_{i}\"\n",
    "    meshID = train_polygon_dict[f\"polygon_{i}\"][0]\n",
    "    nodeID = train_polygon_dict[f\"polygon_{i}\"][1]\n",
    "    polygon_data = Polygon_data(polygonID, meshID, nodeID)\n",
    "    train_polygon_data_list.append(polygon_data)\n",
    "\n",
    "# for i in range(len(eval_polygonID_list)):\n",
    "#     polygonID = f\"polygon_{i}\"\n",
    "#     meshID = eval_polygon_dict[f\"polygon_{i}\"][0]\n",
    "#     nodeID = eval_polygon_dict[f\"polygon_{i}\"][1]\n",
    "#     polygon_data = Polygon_data(polygonID, meshID, nodeID)\n",
    "#     eval_polygon_data_list.append(polygon_data)\n",
    "\n",
    "# for i in range(len(test_polygonID_list)):\n",
    "#     polygonID = f\"polygon_{i}\"\n",
    "#     meshID = test_polygon_dict[f\"polygon_{i}\"][0]\n",
    "#     nodeID = test_polygon_dict[f\"polygon_{i}\"][1]\n",
    "#     polygon_data = Polygon_data(polygonID, meshID, nodeID)\n",
    "#     test_polygon_data_list.append(polygon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Polygon_data"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_polygon_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_polygonID_list, batch_size=64*num_train_mesh, shuffle=True)\n",
    "# eval_data_loader = DataLoader(eval_polygonID_list, batch_size=32, shuffle=True)\n",
    "# test_data_loader = DataLoader(test_polygonID_list, batch_size=64*num_test_mesh, shuffle=True)\n",
    "# for step, data in enumerate(data_loader):\n",
    "#     print(f\"Step {step + 1}:\")\n",
    "#     print(\"==========\")\n",
    "#     print(data)\n",
    "#     print(len(data))\n",
    "#     for i in range(len(data)):\n",
    "#         polygonID = int(data[i].split(\"_\")[-1])\n",
    "#         print(\"polygonID:\",polygonID)\n",
    "#         polygon = data_getter(polygonID, 0)\n",
    "#         print(\"polygon.coordinates:\",polygon.coordinates)\n",
    "#         print(\"polygon.edge_index:\", polygon.edge_index)\n",
    "#         print(\"==========\")\n",
    "#         # plot_mesh(polygon, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "GMSNet(\n",
      "  (shared_mlp): Conv1d(2, 64, kernel_size=(1,), stride=(1,))\n",
      "  (GNorm): GraphNorm(64)\n",
      "  (conv): GCNConv(64, 64)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GMSNet(input_dim=2, feature_dim=64, hidden_channnels=64)\n",
    "model.to(device)\n",
    "print(model)\n",
    "m_loss = MetricLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "criterion = MetricLoss()\n",
    "#criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(\"logs\")\n",
    "# 学習率を調整するスケジューラの設定\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, factor=0.95, verbose=True)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "def train(device):\n",
    "    model.train()\n",
    "\n",
    "    # for name, param in model.GNorm.named_parameters():\n",
    "    #     print(f'{name}:{param.data}')\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    temp = 0\n",
    "    ddd = 0\n",
    "    \n",
    "    for step, data in enumerate(train_data_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        metric_loss = 0\n",
    "        dx_list = []\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)\n",
    "            polygon = normalization(polygon) \n",
    "\n",
    "            # edge_index = polygon.edge_index               \n",
    "            \n",
    "            x = polygon.coordinates.clone().unsqueeze(0).to(device)\n",
    "            # ei = edge_index.to(device)\n",
    "            out = model(x)\n",
    "            \n",
    "            logger.info(\"epoch: {:04}, polygonID: {:04}\".format(epoch, polygonID))\n",
    "            logger.info(\"before\")\n",
    "            with torch.no_grad():\n",
    "                ml = criterion(polygon)\n",
    "                logger.info(\"\")\n",
    "            logger.info(\"after\")\n",
    "            l = criterion(polygon, out[0].cpu())\n",
    "            \n",
    "            logger.info(\"\")\n",
    "            \n",
    "            metric_loss += l\n",
    "            dx_list.append(out[0].cpu())\n",
    "                \n",
    "            #prediction_list[polygonID].append(out[0].cpu())\n",
    "            \n",
    "        loss = metric_loss/len(data)\n",
    "        ddd += len(data)\n",
    "        temp += loss\n",
    "        # print(\"    Loss:\", loss.item(), \"   PolygonID:\", polygonID)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if epoch>10:\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                polygonID = int(data[i].split(\"_\")[-1])\n",
    "\n",
    "                polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)\n",
    "                polygon = normalization(polygon) \n",
    "\n",
    "                polygon.coordinates[0] = polygon.coordinates[0] + dx_list[i]\n",
    "                polygon = denormalization(polygon)\n",
    "                \n",
    "                polygon_meshID = int(train_polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "                mesh = train_mesh_data_list[polygon_meshID]\n",
    "                mesh.coordinates[train_polygon_data_list[polygonID].nodeID[0]] = polygon.coordinates[0]\n",
    "\n",
    "            #print()\n",
    "    loss_ave = temp/ddd\n",
    "    writer.add_scalar(\"loss\", loss_ave, epoch)       \n",
    "    print(loss_ave, epoch)\n",
    "    loss_list.append(temp/ddd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最終的な最適化したメッシュを生成してvtkファイルで出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TensorBoard用のログディレクトリを指定\n",
    "writer = SummaryWriter(log_dir=\"/mnt/log/train\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>) 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:37<30:51, 97.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>) 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [03:16<29:30, 98.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>) 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [04:53<27:44, 97.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>) 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [06:31<26:05, 97.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>) 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [08:08<24:23, 97.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001, grad_fn=<DivBackward0>) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [09:47<22:51, 97.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8201e-05, grad_fn=<DivBackward0>) 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [11:25<21:13, 97.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.9782e-05, grad_fn=<DivBackward0>) 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [13:05<19:45, 98.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.1598e-05, grad_fn=<DivBackward0>) 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [14:41<17:55, 97.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4236e-05, grad_fn=<DivBackward0>) 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [16:22<16:27, 98.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.6586e-05, grad_fn=<DivBackward0>) 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [17:58<14:41, 97.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.0269e-05, grad_fn=<DivBackward0>) 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [19:40<13:14, 99.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4906e-05, grad_fn=<DivBackward0>) 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [21:16<11:27, 98.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0091e-05, grad_fn=<DivBackward0>) 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [22:52<09:45, 97.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6437e-05, grad_fn=<DivBackward0>) 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [24:36<08:17, 99.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2842e-05, grad_fn=<DivBackward0>) 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [26:12<06:33, 98.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0251e-05, grad_fn=<DivBackward0>) 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [27:48<04:53, 97.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7714e-05, grad_fn=<DivBackward0>) 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [29:34<03:20, 100.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5891e-05, grad_fn=<DivBackward0>) 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [31:09<01:38, 98.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4489e-05, grad_fn=<DivBackward0>) 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [32:45<00:00, 98.28s/it]\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for j in range(num_train_mesh):                 # 元のメッシュを保存する\n",
    "        save_mesh(train_mesh_data_list[j], f\"{epoch}_{j}\")\n",
    "\n",
    "for epoch in tqdm(range(num_train_epoch)):\n",
    "    #print(\"epoch:\", epoch)\n",
    "    train(device)\n",
    "    # print(epoch)\n",
    "\n",
    "    # for i in range(num_train_mesh):\n",
    "    #     plot_mesh(train_mesh_data_list[i], f\"{i}\")\n",
    "    for j in range(num_train_mesh):             # 最適化したメッシュを保存する\n",
    "        save_mesh(train_mesh_data_list[j], f\"{epoch+1}_{j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMSNet(\n",
      "  (shared_mlp): Conv1d(2, 64, kernel_size=(1,), stride=(1,))\n",
      "  (GNorm): GraphNorm(64)\n",
      "  (conv): GCNConv(64, 64)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモリの開放をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
