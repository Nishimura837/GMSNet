{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMSNetのモデルを作成する  train,eval,test のデータに分ける\n",
    "### 損失関数の求め方、必要な値が正しく得られているかなど検証してみる\n",
    "2024/5/17 時点での完成版\n",
    "\n",
    "コードをすべて問題なく実行できるようにはなった。\n",
    "最適化されたメッシュを取り出してくることもでき、しっかり更新できている。\n",
    "しかし、損失の値が全く減少せず、むしろ増加していてほぼもともとのメッシュのままの出力が最適化メッシュとして出てきてしまう。\n",
    "ゆえに、最適化できているとは到底言えない。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・各epoch後のメッシュの表示\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・学習率を0.1倍から0.95倍にしてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・シフト切り捨ては推論時飲み使用する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・バッチサイズを変えてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・モデルの出力を0.5倍するなどして移動量を小さくする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・学習時はMetricLossで、最適メッシュを選定するときは違う関数なのはなんで"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シフト切り捨てをやめてみる\n",
    "# トレーニング完了後のトレーニングメッシュを表示してみる\n",
    "# 各epochの終了時にメッシュを可視化してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.norm import GraphNorm\n",
    "import torch_geometric.transforms as T\n",
    "from torch.nn import Linear, InstanceNorm2d, InstanceNorm1d, Conv1d, ReLU, Tanh\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.transforms import FaceToEdge\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from itertools import combinations\n",
    "import vtk\n",
    "import glob\n",
    "from torch_scatter import scatter_mean\n",
    "from tqdm import tqdm\n",
    "# 計算を軽くするためのライブラリ\n",
    "from torch.cuda import empty_cache\n",
    "import gc               # メモリリークを防ぐ\n",
    "\n",
    "from torch import nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 20\n",
    "num_test_epoch = 10\n",
    "num_trial = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, num_files):\n",
    "        None\n",
    "\n",
    "class Mesh(Dataset):\n",
    "    def __init__(self):\n",
    "        self.coordinates = None\n",
    "        self.faces = None\n",
    "\n",
    "class Polygon(Dataset):\n",
    "    def __init__(self, num_node, num_face):\n",
    "        self.parent_meshID = None\n",
    "        self.coordinates = torch.zeros(num_node, 2)\n",
    "        self.faces = torch.zeros(num_face, 3)\n",
    "        self.edges = None\n",
    "        self.d = None\n",
    "        self.Cx = None\n",
    "        self.Cy = None\n",
    "        self.x_min = None\n",
    "        self.y_min = None\n",
    "\n",
    "class PolygonID(Dataset):\n",
    "    def __init__(self, nodeID):\n",
    "        self.nodeID = nodeID\n",
    "        # self.parent_meshID = None\n",
    "\n",
    "class Polygon_data(Dataset):\n",
    "    def __init__(self, polygonID, meshID, nodeID):\n",
    "        self.polygonID = polygonID\n",
    "        self.meshID = meshID\n",
    "        self.nodeID = nodeID\n",
    "\n",
    "class Minibatch(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.edge_index = None\n",
    "        self.batch = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh_polygonID_data(vtk_file_path, polygonID_list, poly_count, polygon_dict, mesh_index):\n",
    "    reader = vtk.vtkDataSetReader()\n",
    "    reader.SetFileName(vtk_file_path)\n",
    "    reader.Update()\n",
    "\n",
    "    data = reader.GetOutput()\n",
    "    \n",
    "    mesh = Mesh()\n",
    "    # 座標情報を取得\n",
    "    points = data.GetPoints()\n",
    "    num_points = points.GetNumberOfPoints()\n",
    "    coordinates = torch.zeros(num_points, 3)\n",
    "    for i in range(num_points):\n",
    "        coordinates[i] = torch.tensor(points.GetPoint(i))\n",
    "\n",
    "    mesh.coordinates = coordinates[:, :2]                        # mesh.coordinates を定義\n",
    "\n",
    "    # 面情報を取得\n",
    "    polys = data.GetPolys()\n",
    "    num_polys = polys.GetNumberOfCells()\n",
    "    mesh.faces = torch.zeros(num_polys, 3, dtype=int)           # mesh.faces を定義\n",
    "\n",
    "    # 各三角形の情報を取得\n",
    "    polys.InitTraversal()\n",
    "    for i in range(num_polys):\n",
    "        cell = vtk.vtkIdList()\n",
    "        if polys.GetNextCell(cell) == 0:\n",
    "            break\n",
    "        mesh.faces[i] = torch.tensor([cell.GetId(0), cell.GetId(1), cell.GetId(2)])\n",
    "        \n",
    "# ------------ mesh のデータを取得完了 -------------------------\n",
    "\n",
    "\n",
    "    # 各セルの各辺の隣接セル数を調べる\n",
    "    edge_neighbors = {}\n",
    "    num_cells = data.GetNumberOfCells()\n",
    "    for cell_index in range(num_cells):\n",
    "        cell = data.GetCell(cell_index)\n",
    "        num_edges = cell.GetNumberOfEdges()\n",
    "\n",
    "        for edge_index in range(num_edges):\n",
    "            edge = cell.GetEdge(edge_index)\n",
    "            edge_points = edge.GetPointIds()\n",
    "\n",
    "            # 辺を構成する点のインデックスを取得\n",
    "            point1_id = edge_points.GetId(0)\n",
    "            point2_id = edge_points.GetId(1)\n",
    "\n",
    "            # 辺を構成する点のインデックスを照準にソート\n",
    "            edge_key = (min(point1_id, point2_id), max(point1_id, point2_id))\n",
    "\n",
    "            # 辺の隣接セル数をカウント\n",
    "            if edge_key in edge_neighbors:\n",
    "                edge_neighbors[edge_key] += 1\n",
    "            else:\n",
    "                edge_neighbors[edge_key] = 1 \n",
    "\n",
    "    boundary_edges = []\n",
    "    # 境界上の辺を特定\n",
    "    for edge_key, num_neighbors in edge_neighbors.items():\n",
    "        if num_neighbors == 1:\n",
    "            boundary_edges.append(edge_key)\n",
    "\n",
    "    # 境界上の辺を構成する頂点の番号を取得\n",
    "    boundary_points = set()     # 集合を表すデータ型、順番を持たず、重複した要素は取り除かれる\n",
    "# ---------------- 自由点かどうかの判定完了 ------------------------\n",
    "    \n",
    "\n",
    "    for edge_key in boundary_edges:\n",
    "        boundary_points.add(edge_key[0])\n",
    "        boundary_points.add(edge_key[1])\n",
    "    \n",
    "    \n",
    "    for pointId in range(num_points):       # pointId:自由点の頂点番号\n",
    "        if pointId in boundary_points:\n",
    "            continue\n",
    "        else:\n",
    "            poly_count += 1\n",
    "            # print(\"pointId:\", pointId)\n",
    "        mask = (mesh.faces == pointId)\n",
    "        if mask.any():\n",
    "            count = torch.sum(mask).item()\n",
    "        num_node = count + 1\n",
    "        num_face = count\n",
    "        polygon_number = poly_count - 1 \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        polygon_i = f\"polygon_{polygon_number}\"\n",
    "        # print(polygon_i)\n",
    "        polygon_i = Polygon(num_node, num_face)\n",
    "        \n",
    "        element_to_check = pointId\n",
    "        polygon_i.face = mesh.faces[(mesh.faces == element_to_check).any(dim=1)]\n",
    "        # print(polygon_i.face)\n",
    "\n",
    "        polygon_i.nodeId = set()\n",
    "        for i in range(len(polygon_i.face)):\n",
    "            polygon_i.nodeId.add(polygon_i.face[i, 0].item())\n",
    "            polygon_i.nodeId.add(polygon_i.face[i, 1].item())\n",
    "            polygon_i.nodeId.add(polygon_i.face[i, 2].item())\n",
    "        sorted_nodeId = sorted(polygon_i.nodeId)\n",
    "        polygon_i.nodeID = torch.tensor(list(sorted_nodeId))\n",
    "        \n",
    "        point_id_index = (polygon_i.nodeID == pointId).nonzero().item()\n",
    "\n",
    "        value_to_move = polygon_i.nodeID[point_id_index]\n",
    "        polygon_i.nodeID = torch.cat((value_to_move.unsqueeze(0), polygon_i.nodeID[polygon_i.nodeID != pointId]))\n",
    "        # print(polygon_i.nodeID)\n",
    "        setattr(polygon_i, \"parent_meshID\", mesh)\n",
    "        polygonID_list.append(f\"polygon_{polygon_number}\")\n",
    "\n",
    "        keyword = f\"polygon_{polygon_number}\"\n",
    "        valiables = (f\"mesh_{mesh_index}\", polygon_i.nodeID)\n",
    "        polygon_dict[keyword] = valiables\n",
    "\n",
    "    # --------- polygon.nodeID の取得完了 -------------\n",
    "    return mesh, polygonID_list, poly_count, polygon_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh_polygon_dataset(vtk_files):\n",
    "    num_vtk_files = len(vtk_files)\n",
    "    polygonID_list = []\n",
    "    mesh_data_list = []\n",
    "    poly_count = 0\n",
    "    polygon_dict = {}\n",
    "    # ファイルに順にアクセスする\n",
    "    for i in range(num_vtk_files):\n",
    "        # print(\"File Name:\", vtk_files[i])\n",
    "        mesh, polygonID_list, poly_count, polygon_dict = create_mesh_polygonID_data(vtk_files[i], polygonID_list, poly_count, polygon_dict, i)\n",
    "        mesh_data_list.append(mesh)\n",
    "    return mesh_data_list, polygonID_list, polygon_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下、i はpolygon番号で座標と面情報を取得することができる\n",
    "face_to_edge = T.FaceToEdge(remove_faces=False)\n",
    "def data_getter(polygonID, num_mesh_data_list, mesh_data_list, polygon_data_list):\n",
    "    \n",
    "    polygon_meshID = int(polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "    mesh = mesh_data_list[polygon_meshID]\n",
    "    \n",
    "    num_node = len(polygon_data_list[polygonID].nodeID)\n",
    "    num_face = num_node - 1 \n",
    "    polygon_i = Polygon(num_node, num_face)\n",
    "\n",
    "    # print(polygon_data_list[polygonID].nodeID)      # polygon に属する頂点の番号\n",
    "\n",
    "    polygon_i.coordinates = mesh.coordinates[polygon_data_list[polygonID].nodeID]     # polygonの座標\n",
    "    # print(polygon_i.coordinates)\n",
    "\n",
    "    # print(polygon_i.faces)\n",
    "\n",
    "    # polygon_i.faces を取得するコード\n",
    "    \n",
    "    element_to_check = polygon_data_list[polygonID].nodeID[0]\n",
    "    polygon_i.face = mesh.faces[(mesh.faces == element_to_check).any(dim=1)]\n",
    "\n",
    "    indices = torch.nonzero(torch.isin(polygon_i.face, polygon_data_list[polygonID].nodeID))\n",
    "    for idx in range(indices.size(0)):\n",
    "        row_idx, col_idx = indices[idx]\n",
    "        value_to_replace = polygon_i.face[row_idx, col_idx]\n",
    "        polygon_i.face[row_idx, col_idx] = (polygon_data_list[polygonID].nodeID == value_to_replace).nonzero().item()\n",
    "    polygon_i.faces = polygon_i.face.long()\n",
    "\n",
    "    # 各行の三角形からエッジを抽出してedge_indexを構築\n",
    "    edges = torch.cat([ polygon_i.faces[:, [0, 1]],\n",
    "                        polygon_i.faces[:, [1, 2]],\n",
    "                        polygon_i.faces[:, [2, 0]]], dim=0)\n",
    "\n",
    "    # エッジのインデックスをソートして重複を削除\n",
    "    edge_index = torch.sort(edges, dim=1).values\n",
    "    edge_index = torch.tensor(sorted(edge_index.numpy().tolist())).unique(dim=0)\n",
    "    polygon_i.edge_index = torch.transpose(edge_index, 0, 1)\n",
    "    return polygon_i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メッシュをプロットする関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(mesh, title):\n",
    "\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect=\"equal\")\n",
    "\n",
    "    # 描画するメッシュの頂点をプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'bo')  # 頂点を青色の点でプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'k-')  # 辺を黒色の線でプロット\n",
    "\n",
    "    # 各三角形をプロット\n",
    "    for face in faces:\n",
    "        v0, v1, v2 = vertices[face]\n",
    "        v0_np = v0.detach().numpy()\n",
    "        v1_np = v1.detach().numpy()\n",
    "        v2_np = v2.detach().numpy()\n",
    "        ax.plot([v0_np[0], v1_np[0], v2_np[0], v0_np[0]], [v0_np[1], v1_np[1], v2_np[1], v0_np[1]], 'b-')  # 三角形を赤色の線でプロット\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.001)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.001)\n",
    "\n",
    "    # plt.xlim(-300, 150)\n",
    "    # plt.ylim(-200, 1400)\n",
    "    # plt.savefig(f\"/mnt/{title}.png\", format=\"png\")\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mesh(mesh, title):\n",
    "\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect=\"equal\")\n",
    "\n",
    "    # 描画するメッシュの頂点をプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'bo')  # 頂点を青色の点でプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'k-')  # 辺を黒色の線でプロット\n",
    "\n",
    "    # 各三角形をプロット\n",
    "    for face in faces:\n",
    "        v0, v1, v2 = vertices[face]\n",
    "        v0_np = v0.detach().numpy()\n",
    "        v1_np = v1.detach().numpy()\n",
    "        v2_np = v2.detach().numpy()\n",
    "        ax.plot([v0_np[0], v1_np[0], v2_np[0], v0_np[0]], [v0_np[1], v1_np[1], v2_np[1], v0_np[1]], 'b-')  # 三角形を赤色の線でプロット\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.001)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.001)\n",
    "\n",
    "    # plt.xlim(-300, 150)\n",
    "    # plt.ylim(-200, 1400)\n",
    "    plt.savefig(f\"/mnt/Saved_mesh/{title}.png\", format=\"png\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meshデータからvtkファイルを出力する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtk_output(mesh, title):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    num_vertices = len(vertices)\n",
    "    num_faces = len(faces)\n",
    "\n",
    "\n",
    "    # vertices を３次元に戻す\n",
    "    z_column = torch.zeros(vertices.shape[0], 1)\n",
    "    vertices = torch.cat((vertices, z_column), dim=1)\n",
    "\n",
    "    with open(f\"/mnt/optimized_data/{title}.vtk\", \"w\") as f:\n",
    "        f.write(\"# vtk DataFile Version 2.0\\n\")\n",
    "        f.write(\"FOR TEST\\n\")\n",
    "        f.write(\"ASCII\\n\")\n",
    "        f.write(\"DATASET POLYDATA\\n\")\n",
    "\n",
    "        f.write(\"POINTS {} float\\n\".format(num_vertices))\n",
    "        for vertex in vertices:\n",
    "            f.write(\"{:.15f} {:.15f} {:.15f}\\n\".format(*vertex))\n",
    "\n",
    "        f.write(\"\\nPOLYGONS {} {}\\n\".format(num_faces, num_faces * 4))\n",
    "        for face in faces:\n",
    "            f.write(\"3 \")\n",
    "            f.write(\" \".join(str(idx.item()) for idx in face))\n",
    "            f.write(\"\\n\")\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(polygon):\n",
    "    vertices = polygon.coordinates\n",
    "    normalized_vertices = vertices.clone()\n",
    "    centered_vertices = vertices.clone()\n",
    "    # print(vertices)\n",
    "\n",
    "    max_x = torch.max(vertices[:,0])\n",
    "    min_x = torch.min(vertices[:,0])\n",
    "    max_y = torch.max(vertices[:,1])\n",
    "    min_y = torch.min(vertices[:,1])\n",
    "\n",
    "    polygon.d = torch.max(max_x - min_x, max_y - min_y)\n",
    "    polygon.x_min = min_x\n",
    "    polygon.y_min = min_y\n",
    "\n",
    "    normalized_vertices = (vertices - torch.tensor([polygon.x_min, polygon.y_min])) / polygon.d\n",
    "\n",
    "    \n",
    "    polygon.Cx = normalized_vertices[0,0].item()\n",
    "    polygon.Cy = normalized_vertices[0,1].item()   \n",
    "\n",
    "    centered_vertices = normalized_vertices - torch.tensor([polygon.Cx, polygon.Cy])\n",
    "    polygon.coordinates = centered_vertices\n",
    "    \n",
    "    # print(\"Normalized polygon:\", vertices)\n",
    "\n",
    "    return polygon\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(polygon):\n",
    "    vertices = polygon.coordinates\n",
    "    shifted_vertices = vertices.clone()\n",
    "    denormalized_vertices = vertices.clone()\n",
    "    \n",
    "    shifted_vertices = vertices + torch.tensor([polygon.Cx, polygon.Cy])\n",
    "        \n",
    "\n",
    "    denormalized_vertices = polygon.d * shifted_vertices + torch.tensor([polygon.x_min, polygon.y_min])\n",
    "    polygon.coordinates = denormalized_vertices\n",
    "    return polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='metric_loss.log',\n",
    "    level=logging.DEBUG, \n",
    "    format='%(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MetricLoss:\n",
    "    def select_vertices(self, vertices, face):\n",
    "        v0 = vertices[face[0]].clone()\n",
    "        v1 = vertices[face[1]].clone()\n",
    "        v2 = vertices[face[2]].clone()\n",
    "        return v0, v1, v2 \n",
    "\n",
    "    def edge_length(self, v0, v1, v2):\n",
    "        l1 = torch.sqrt((v0[0] - v1[0])**2 + (v0[1] - v1[1])**2)\n",
    "        l2 = torch.sqrt((v1[0] - v2[0])**2 + (v1[1] - v2[1])**2)\n",
    "        l3 = torch.sqrt((v2[0] - v0[0])**2 + (v2[1] - v0[1])**2)\n",
    "\n",
    "        return l1, l2, l3\n",
    "\n",
    "    def face_area(self, polygon, l1, l2, l3):\n",
    "        \n",
    "        s = 0.5*(l1 + l2 + l3)\n",
    "        #print(round(l1.item(),5), round(l2.item(),5), round(l3.item(),5), round(s.item(),5), round((s*(s-l1)*(s-l2)*(s-l3)).item(),5))\n",
    "        temp = s*(s-l1)*(s-l2)*(s-l3)\n",
    "        #temp.register_hook(print_grad)\n",
    "        logger.info(\"    s, in_sqrt: {}, {}\".format(s.item(), temp.item()))\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            face_area = torch.sqrt(temp)\n",
    "            \n",
    "        except Exception as e:\n",
    "            \n",
    "            SimplePolygonGenerator.polygon_visualizer(polygon)\n",
    "            \n",
    "            print(\"An error occurred\")\n",
    "            print(\"Value of temp:\", temp)\n",
    "            print(l1.item(), l2.item(), l3.item())\n",
    "            \n",
    "            raise\n",
    "        \n",
    "        #face_area = torch.sqrt(temp)\n",
    "        #face_area.register_hook(print_grad)\n",
    "        return face_area\n",
    "\n",
    "    def compute_loss(self, polygon, vertices, face, dx):\n",
    "        v0, v1, v2 = self.select_vertices(vertices, face)\n",
    "        if dx is not None:\n",
    "            if face[0]==0:\n",
    "                v0 = v0 + dx\n",
    "            elif face[1]==0:\n",
    "                v1 += v1 + dx\n",
    "            elif face[2]==0:\n",
    "                v2 += v2 + dx\n",
    "        #print(v0, v1, v2)\n",
    "        logger.info(\"    v0: ({}, {})\".format(v0[0].item(), v0[1].item()))\n",
    "        logger.info(\"    v1: ({}, {})\".format(v1[0].item(), v1[1].item()))\n",
    "        logger.info(\"    v2: ({}, {})\".format(v2[0].item(), v2[1].item()))\n",
    "        l1, l2, l3 = self.edge_length(v0, v1, v2)\n",
    "        logger.info(\"    l1, l2, l3:  {}, {}, {}\".format(l1.item(), l2.item(), l3.item()))\n",
    "        s = self.face_area(polygon, l1, l2, l3)\n",
    "        #print(s.item(), l1.item(), l2.item(), l3.item())\n",
    "\n",
    "        #q = (l1**2 + l2**2 + l3**2)/(4.0*torch.sqrt(torch.tensor(3.))*s+1.0)\n",
    "        \n",
    "        #loss = 1 - 1/q\n",
    "        #print(q.item(), loss.item())\n",
    "        \n",
    "        #q = q.clone().detach().requires_grad_(True)\n",
    "        loss = 1-(4.0*torch.sqrt(torch.tensor(3.))*s)/(l1**2 + l2**2 + l3**2)\n",
    "        logger.info(\"    area, loss: {}, {}\".format(s.item(), loss.item()))\n",
    "        logger.info(\"\")\n",
    "        #print(loss.item())\n",
    "        #print(\"\")\n",
    "        \n",
    "        #loss.register_hook(print_grad)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def __call__(self, polygon, dx=None):\n",
    "        vertices = polygon.coordinates\n",
    "        faces = polygon.faces\n",
    "        loss = 0 \n",
    "        #print(vertices)\n",
    "        #print(dx)\n",
    "        for face in faces:\n",
    "            loss = loss + self.compute_loss(polygon, vertices, face, dx)\n",
    "        \n",
    "        metric_loss = loss/(len(polygon.coordinates[:,0])-1) #.clone().detach().requires_grad_(True))\n",
    "        return metric_loss\n",
    "    \n",
    "    \n",
    "def print_grad(grad):\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meshデータからq_hatを求める関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_hat(mesh):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.faces\n",
    "    r_list = []\n",
    "    alpha_list = []\n",
    "    beta_list = []\n",
    "\n",
    "    for face in faces:\n",
    "        # a(最小角)と b(最大角)を求める\n",
    "\n",
    "        angles = []\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "\n",
    "        # 余弦定理から各角度の余弦値を計算\n",
    "        cos_alpha = (l2**2 + l3**2 - l1**2) / (2*l2*l3)\n",
    "        cos_beta = (l1**2 + l3**2 - l2**2) / (2*l1*l3)\n",
    "        cos_gamma = (l1**2 + l2**2 - l3**2) / (2*l1*l2)\n",
    "        # 余弦値から角度を計算して個度法に変換\n",
    "        alpha = torch.acos(cos_alpha) * 180 / np.pi\n",
    "        beta = torch.acos(cos_beta) * 180 / np.pi\n",
    "        gamma = torch.acos(cos_gamma) * 180 / np.pi\n",
    "\n",
    "        angles.append(alpha)\n",
    "        angles.append(beta)\n",
    "        angles.append(gamma)\n",
    "\n",
    "        min_angle = min(angles)\n",
    "        max_angle = max(angles)\n",
    "\n",
    "        alpha_list.append(min_angle)\n",
    "        beta_list.append(max_angle)\n",
    "\n",
    "\n",
    "\n",
    "        # 1/q = r を求める\n",
    "\n",
    "        r = 1 - m_loss.compute_loss(vertices, face) \n",
    "        r_list.append(r)\n",
    "\n",
    "    a_mean = sum(alpha_list) / len(alpha_list)\n",
    "    a_min = min(alpha_list)\n",
    "    b_mean = sum(beta_list) / len(beta_list)\n",
    "    b_max = max(beta_list)\n",
    "    r_mean = sum(r_list) / len(r_list)\n",
    "    r_min = min(r_list)\n",
    "\n",
    "    q_hat = (((a_mean + a_min + 120 - b_max - b_mean)/60) + r_mean + r_min) / 6\n",
    "\n",
    "    return q_hat\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スターポリゴンの中から外側に自由点が移動したときに自由点の移動量を半分にしてもう一度外に行っていないか検証する\n",
    "自由点が外に行かないことを確認したあとのスターポリゴンを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(polygon, polygonID):\n",
    "    # print(\"polygonID:\", polygonID)\n",
    "    vertices = polygon.coordinates\n",
    "    \n",
    "    edge_index = polygon.edge_index\n",
    "    \n",
    "    return_value = True\n",
    "    while return_value == True:   \n",
    "\n",
    "        for i in range(1, len(vertices[:,0])):\n",
    "            point1 = torch.tensor([0.0, 0.0])\n",
    "            point2 = vertices[0]\n",
    "            point3 = vertices[i]\n",
    "            \n",
    "            pos_i = torch.where(edge_index[0] == i)\n",
    "            pos_i = pos_i[0]\n",
    "            # print(\"edge_index[0]\", edge_index[0])\n",
    "            \n",
    "            for j in range(len(pos_i)):\n",
    "                if edge_index[1, pos_i[j]] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    val_pos_i = edge_index[1, pos_i[j]]\n",
    "                    break\n",
    "\n",
    "            point4 = vertices[val_pos_i]\n",
    "\n",
    "\n",
    "            a1 = 0\n",
    "            b1 = 0\n",
    "            a2 = 0\n",
    "            b2 = 0\n",
    "            check1 = 0\n",
    "            check2 = 0\n",
    "            check3 = 0\n",
    "            check4 = 0\n",
    "            x1 = point1[0]\n",
    "            y1 = point1[1]\n",
    "            x2 = point2[0]\n",
    "            y2 = point2[1]\n",
    "            x3 = point3[0]\n",
    "            y3 = point3[1]\n",
    "            x4 = point4[0]\n",
    "            y4 = point4[1]\n",
    "            a1 = (y1 - y2)/(x1-x2)\n",
    "            b1 = y1 - (a1*x1)\n",
    "            a2 = (y3 - y4)/(x3-x4)\n",
    "            b2 = y3 - (a2*x3)\n",
    "            check1 = (a1*x3) - y3 + b1 \n",
    "            check2 = (a1*x4) - y4 + b1    # point1,2を通る直線に対してpoint3,4を結ぶ線分が交差しているか\n",
    "            check3 = (a2*x1) - y1 + b2\n",
    "            check4 = (a2*x2) - y2 + b2    # point3,4を通る直線に対してpoint1,2を結ぶ線分が交差しているか\n",
    "            # print(\"1:\",check1,\"2:\",check2,\"3:\",check3,\"4:\",check4)\n",
    "            del a1, a2, b1, b2, x1, x2, x3, x4, y1, y2, y3, y4 \n",
    "\n",
    "            if (check1*check2) <= 0 and (check3*check4) <= 0 :\n",
    "                return_value = True\n",
    "                # print(\"Out_of_StarPolygon\")\n",
    "                vertices[0] = 0.5*vertices[0]\n",
    "                polygon.coordinates[0] = vertices[0]\n",
    "                break\n",
    "            else:\n",
    "                return_value = False\n",
    "                continue       \n",
    "            \n",
    "        \n",
    "    # plot_mesh(polygon, \"polygon_checked\")\n",
    "               \n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 隠れ層のノード数は何にするか未定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMSNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim, hidden_channnels):\n",
    "        \n",
    "        super(GMSNet, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.shared_mlp = Conv1d(input_dim, feature_dim, kernel_size=1)\n",
    "        self.GNorm = GraphNorm(feature_dim, feature_dim)\n",
    "        self.conv = GCNConv(feature_dim, feature_dim)\n",
    "        self.fc1 = Linear(feature_dim, hidden_channnels)\n",
    "        #self.ISNorm = InstanceNorm1d(hidden_channnels, affine=True)\n",
    "        self.fc2 = Linear(hidden_channnels, input_dim)\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):       \n",
    "        \n",
    "        # feature\n",
    "        x = torch.permute(x, (0, 2, 1))\n",
    "        x = self.shared_mlp(x)\n",
    "        feature = self.relu(x)\n",
    "        feature = torch.permute(feature, (0, 2, 1))\n",
    "        \n",
    "        # GNN\n",
    "        # x = self.GNorm(feature)\n",
    "        x = self.relu(feature)\n",
    "        gnn_feature = self.conv(x, edge_index) + feature\n",
    "        \n",
    "        # MLP\n",
    "        target_feature = gnn_feature.mean(dim=1)\n",
    "        mlp_midlayer = self.fc1(target_feature)\n",
    "        x = self.relu(mlp_midlayer)\n",
    "        x = self.fc2(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        x = 0.1*x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_mesh: 3\n"
     ]
    }
   ],
   "source": [
    "# フォルダ内のすべてのvtkファイルにアクセスする\n",
    "train_vtk_files = glob.glob(\"/mnt/new_train_data/*.vtk\")\n",
    "eval_vtk_files = glob.glob(\"/mnt/Eval_Data/*.vtk\")\n",
    "test_vtk_files = glob.glob(\"/mnt/Test_Data/*vtk\")\n",
    "\n",
    "num_train_mesh = len(train_vtk_files)\n",
    "num_test_mesh = len(test_vtk_files)\n",
    "print(\"num_train_mesh:\", num_train_mesh)\n",
    "train_mesh_data_list, train_polygonID_list, train_polygon_dict = create_mesh_polygon_dataset(train_vtk_files)\n",
    "eval_mesh_data_list, eval_polygonID_list, eval_polygon_dict = create_mesh_polygon_dataset(eval_vtk_files)\n",
    "test_mesh_data_list, test_polygonID_list, test_polygon_dict = create_mesh_polygon_dataset(test_vtk_files)\n",
    "\n",
    "\n",
    "    \n",
    "# ポリゴンデータを格納するリストを作成\n",
    "train_polygon_data_list = []\n",
    "eval_polygon_data_list = []\n",
    "test_polygon_data_list = []\n",
    "\n",
    "for i in range(len(train_polygonID_list)):\n",
    "    polygonID = f\"polygon_{i}\"\n",
    "    meshID = train_polygon_dict[f\"polygon_{i}\"][0]\n",
    "    nodeID = train_polygon_dict[f\"polygon_{i}\"][1]\n",
    "    polygon_data = Polygon_data(polygonID, meshID, nodeID)\n",
    "    train_polygon_data_list.append(polygon_data)\n",
    "\n",
    "for i in range(len(eval_polygonID_list)):\n",
    "    polygonID = f\"polygon_{i}\"\n",
    "    meshID = eval_polygon_dict[f\"polygon_{i}\"][0]\n",
    "    nodeID = eval_polygon_dict[f\"polygon_{i}\"][1]\n",
    "    polygon_data = Polygon_data(polygonID, meshID, nodeID)\n",
    "    eval_polygon_data_list.append(polygon_data)\n",
    "\n",
    "for i in range(len(test_polygonID_list)):\n",
    "    polygonID = f\"polygon_{i}\"\n",
    "    meshID = test_polygon_dict[f\"polygon_{i}\"][0]\n",
    "    nodeID = test_polygon_dict[f\"polygon_{i}\"][1]\n",
    "    polygon_data = Polygon_data(polygonID, meshID, nodeID)\n",
    "    test_polygon_data_list.append(polygon_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Polygon_data"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_polygon_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_polygonID_list, batch_size=64*num_train_mesh, shuffle=True)\n",
    "# eval_data_loader = DataLoader(eval_polygonID_list, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_polygonID_list, batch_size=64*num_test_mesh, shuffle=True)\n",
    "# for step, data in enumerate(data_loader):\n",
    "#     print(f\"Step {step + 1}:\")\n",
    "#     print(\"==========\")\n",
    "#     print(data)\n",
    "#     print(len(data))\n",
    "#     for i in range(len(data)):\n",
    "#         polygonID = int(data[i].split(\"_\")[-1])\n",
    "#         print(\"polygonID:\",polygonID)\n",
    "#         polygon = data_getter(polygonID, 0)\n",
    "#         print(\"polygon.coordinates:\",polygon.coordinates)\n",
    "#         print(\"polygon.edge_index:\", polygon.edge_index)\n",
    "#         print(\"==========\")\n",
    "#         # plot_mesh(polygon, \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "GMSNet(\n",
      "  (shared_mlp): Conv1d(2, 64, kernel_size=(1,), stride=(1,))\n",
      "  (GNorm): GraphNorm(64)\n",
      "  (conv): GCNConv(64, 64)\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (tanh): Tanh()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GMSNet(input_dim=2, feature_dim=64, hidden_channnels=64)\n",
    "model.to(device)\n",
    "print(model)\n",
    "m_loss = MetricLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "criterion = MetricLoss()\n",
    "#criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('mnt/model_weight.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(log_dir=\"mnt/logs/\" + datetime.datetime.now()strftime(\"%Y%m%d-%H%M%S\"))\n",
    "# 学習率を調整するスケジューラの設定\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, factor=0.95, verbose=True)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "def train_(device):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for step, data in enumerate(train_data_loader):\n",
    "        empty_cache()\n",
    "        gc.collect()\n",
    "        # print(f\"Step {step + 1}:\")\n",
    "        # print(\"==========\")\n",
    "        # print(data)\n",
    "        # print(len(data))\n",
    "        minibatch = Minibatch()\n",
    "        minibatch_coordinates = []\n",
    "        all_edge_index_1 = []\n",
    "        all_edge_index_2 = []\n",
    "        batch_list = []\n",
    "        num_dis = 0\n",
    "        metric_loss_list = []\n",
    "        for i in range(len(data)):\n",
    "            # gc.collect()\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            # print(\"polygonID:\",polygonID)\n",
    "            polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)                                                                     # 270MiB\n",
    "            # plot_mesh(polygon, \"\")      #############################################\n",
    "\n",
    "            # polygon を正規化する\n",
    "            polygon = normalization(polygon)                                                                        # 900MiB\n",
    "            \n",
    "            # plot_mesh(polygon, \"\")      #############################################\n",
    "\n",
    "            # print(\"polygon.coordinates:\",polygon.coordinates)\n",
    "            edge_index = polygon.edge_index + num_dis                               \n",
    "            # print(\"polygon.edge_index:\", edge_index)\n",
    "            all_edge_index_1.append(edge_index[0])\n",
    "            all_edge_index_2.append(edge_index[1])\n",
    "            num_dis = num_dis + len(polygon.coordinates)\n",
    "            # print(\"==========\")\n",
    "            minibatch_coordinates.append(polygon.coordinates)\n",
    "            batch_i = torch.tensor([i]*len(polygon.coordinates))                    \n",
    "            batch_list.append(batch_i)\n",
    "            \n",
    "        # print(minibatch_coordinates)\n",
    "        minibatch.x = torch.cat(minibatch_coordinates, dim=0)\n",
    "        \n",
    "        # print(minibatch.x)\n",
    "        # print(\"minibatch.x.size:\", minibatch.x.size())\n",
    "        edge_index_1 = torch.cat(all_edge_index_1, dim=-1)\n",
    "        edge_index_2 = torch.cat(all_edge_index_2, dim=-1)\n",
    "        minibatch.edge_index = torch.cat([edge_index_1.unsqueeze(0), edge_index_2.unsqueeze(0)], dim=0)\n",
    "        # print(minibatch.edge_index)\n",
    "        # print(\"minibatch.edge_index.size:\", minibatch.edge_index.size())\n",
    "        minibatch.batch = torch.cat(batch_list, dim=0)\n",
    "        # print(minibatch.batch)\n",
    "        # print(\"minibatch.batch.size:\", minibatch.batch.size())\n",
    "\n",
    "        # すべてのデータをGPU上に移動する\n",
    "        minibatch.x = minibatch.x.to(device)\n",
    "        minibatch.edge_index = minibatch.edge_index.to(device)\n",
    "        minibatch.batch = minibatch.batch.to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        out = model(minibatch.x, minibatch.edge_index)                                     # 333MiB\n",
    "        # print(\"out:\", out)\n",
    "        # print(\"out:\", out.size())\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            # gc.collect()\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)                                                             # 225MiB\n",
    "            polygon = normalization(polygon)                                                                # 932MiB\n",
    "\n",
    "            metric_loss = criterion(polygon)\n",
    "            metric_loss = criterion(polygon, out[i].cpu())\n",
    "            \n",
    "            polygon.coordinates[0] = out[i]\n",
    "\n",
    "            metric_loss_list.append(metric_loss)\n",
    "\n",
    "        loss = (sum(metric_loss_list) / len(metric_loss_list))      # .requires_grad_(True)\n",
    "        print(\"Loss:\", loss)\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "        # print(\"loss:\", loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(data)):\n",
    "                # gc.collect()\n",
    "                polygonID = int(data[i].split(\"_\")[-1])\n",
    "                polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)                                                             # 225MiB\n",
    "                polygon = normalization(polygon)    \n",
    "                \n",
    "                polygon.coordinates[0] = polygon.coordinates[0] + out[i]\n",
    "                \n",
    "                polygon_meshID = int(train_polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "                mesh = train_mesh_data_list[polygon_meshID]\n",
    "\n",
    "                mesh.coordinates[train_polygon_data_list[polygonID].nodeID[0]] = polygon.coordinates[0]\n",
    "        \n",
    "        del out\n",
    "        \n",
    "        loss.detach()               # 計算グラフを切り離し、不要な計算グラフが保持されることを防ぐ\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # ステップごとに損失をログに記録\n",
    "        writer.add_scalar(\"/mnt/logs\", loss.item(), global_step=len(train_data_loader)*epoch + step)\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "    \n",
    "    val_loss = sum(loss_list)/ len(loss_list)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "def train(device):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    temp = 0\n",
    "    ddd = 0\n",
    "    \n",
    "    for step, data in enumerate(train_data_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        metric_loss = 0\n",
    "        dx_list = []\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)\n",
    "            polygon = normalization(polygon) \n",
    "\n",
    "            edge_index = polygon.edge_index               \n",
    "            \n",
    "            x = polygon.coordinates.clone().unsqueeze(0).to(device)\n",
    "            ei = edge_index.to(device)\n",
    "            out = model(x, ei)\n",
    "            \n",
    "            logger.info(\"epoch: {:04}, polygonID: {:04}\".format(epoch, polygonID))\n",
    "            logger.info(\"before\")\n",
    "            with torch.no_grad():\n",
    "                ml = criterion(polygon)\n",
    "                logger.info(\"\")\n",
    "            logger.info(\"after\")\n",
    "            l = criterion(polygon, out[0].cpu())\n",
    "            \n",
    "            logger.info(\"\")\n",
    "            \n",
    "            metric_loss += l\n",
    "            dx_list.append(out[0].cpu())\n",
    "                \n",
    "            #prediction_list[polygonID].append(out[0].cpu())\n",
    "            \n",
    "        loss = metric_loss/len(data)\n",
    "        ddd += len(data)\n",
    "        temp += loss\n",
    "        print(\"    Loss:\", loss.item(), polygonID)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if epoch>10:\n",
    "        for i in range(len(data)):\n",
    "\n",
    "            with torch.no_grad():\n",
    "                polygonID = int(data[i].split(\"_\")[-1])\n",
    "\n",
    "                polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)\n",
    "                polygon = normalization(polygon) \n",
    "\n",
    "                polygon.coordinates[0] = polygon.coordinates[0] + dx_list[i]\n",
    "                polygon = denormalization(polygon)\n",
    "                \n",
    "                polygon_meshID = int(train_polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "                mesh = train_mesh_data_list[polygon_meshID]\n",
    "                mesh.coordinates[train_polygon_data_list[polygonID].nodeID[0]] = polygon.coordinates[0]\n",
    "\n",
    "            #print()\n",
    "    writer.add_scalar(\"loss\", temp/ddd, epoch)        \n",
    "    loss_list.append(temp/ddd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最終的な最適化したメッシュを生成してvtkファイルで出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.04175681248307228 143\n",
      "    Loss: 0.04238526523113251 333\n",
      "    Loss: 0.038668349385261536 1126\n",
      "    Loss: 0.040344804525375366 1101\n",
      "    Loss: 0.041604939848184586 1303\n",
      "    Loss: 0.04191674664616585 1424\n",
      "    Loss: 0.03896954283118248 1007\n",
      "    Loss: 0.03568774461746216 758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:18<24:56, 78.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.0428156740963459 873\n",
      "    Loss: 0.04394881799817085 896\n",
      "    Loss: 0.039640843868255615 668\n",
      "    Loss: 0.04190824553370476 827\n",
      "    Loss: 0.04192792996764183 1092\n",
      "    Loss: 0.04024863988161087 94\n",
      "    Loss: 0.0423746258020401 466\n",
      "    Loss: 0.04171696677803993 327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [02:37<23:36, 78.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.044323358684778214 917\n",
      "    Loss: 0.042368095368146896 1436\n",
      "    Loss: 0.04675270989537239 757\n",
      "    Loss: 0.04548662528395653 752\n",
      "    Loss: 0.04682115837931633 1051\n",
      "    Loss: 0.044623490422964096 114\n",
      "    Loss: 0.04044599086046219 614\n",
      "    Loss: 0.0493045449256897 1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [03:55<22:11, 78.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.046055883169174194 175\n",
      "    Loss: 0.04889382794499397 200\n",
      "    Loss: 0.046966105699539185 1024\n",
      "    Loss: 0.04705004021525383 697\n",
      "    Loss: 0.04711902141571045 802\n",
      "    Loss: 0.046977534890174866 1483\n",
      "    Loss: 0.04646129533648491 457\n",
      "    Loss: 0.05063688009977341 1362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [05:13<20:50, 78.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.04574117437005043 1395\n",
      "    Loss: 0.050673361867666245 326\n",
      "    Loss: 0.04631051793694496 385\n",
      "    Loss: 0.05341792106628418 1259\n",
      "    Loss: 0.051485057920217514 1464\n",
      "    Loss: 0.04865396395325661 351\n",
      "    Loss: 0.0474836491048336 61\n",
      "    Loss: 0.04818778112530708 870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [06:30<19:25, 77.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.0475563108921051 932\n",
      "    Loss: 0.04984183982014656 1434\n",
      "    Loss: 0.049684058874845505 983\n",
      "    Loss: 0.05084561929106712 213\n",
      "    Loss: 0.049634456634521484 465\n",
      "    Loss: 0.04864880070090294 927\n",
      "    Loss: 0.0465807281434536 712\n",
      "    Loss: 0.05103706195950508 1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [07:48<18:12, 78.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.051886752247810364 527\n",
      "    Loss: 0.048116061836481094 612\n",
      "    Loss: 0.04457961022853851 191\n",
      "    Loss: 0.04859668388962746 297\n",
      "    Loss: 0.0474238246679306 276\n",
      "    Loss: 0.04698377847671509 536\n",
      "    Loss: 0.045717377215623856 1465\n",
      "    Loss: 0.04395141825079918 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [09:07<16:57, 78.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.04607630893588066 846\n",
      "    Loss: 0.04580572247505188 1020\n",
      "    Loss: 0.04288668930530548 506\n",
      "    Loss: 0.04519975557923317 789\n",
      "    Loss: 0.0411691851913929 346\n",
      "    Loss: 0.0437016487121582 1415\n",
      "    Loss: 0.0420299731194973 900\n",
      "    Loss: 0.04062485694885254 1163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [10:24<15:34, 77.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.04233631491661072 1103\n",
      "    Loss: 0.038255758583545685 1093\n",
      "    Loss: 0.037721071392297745 873\n",
      "    Loss: 0.03603743389248848 343\n",
      "    Loss: 0.03839532285928726 707\n",
      "    Loss: 0.039037831127643585 1058\n",
      "    Loss: 0.03541308268904686 412\n",
      "    Loss: 0.038366351276636124 1379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [11:45<14:26, 78.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.035675615072250366 975\n",
      "    Loss: 0.03419075533747673 302\n",
      "    Loss: 0.034629445523023605 901\n",
      "    Loss: 0.032403480261564255 1350\n",
      "    Loss: 0.03371826186776161 293\n",
      "    Loss: 0.031342171132564545 596\n",
      "    Loss: 0.031205808743834496 252\n",
      "    Loss: 0.03118315525352955 1216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [13:06<13:15, 79.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.03048202581703663 377\n",
      "    Loss: 0.02822781912982464 1317\n",
      "    Loss: 0.031425464898347855 478\n",
      "    Loss: 0.02820245362818241 736\n",
      "    Loss: 0.02983812429010868 667\n",
      "    Loss: 0.027293389663100243 1277\n",
      "    Loss: 0.026985546573996544 119\n",
      "    Loss: 0.02702704444527626 832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [14:23<11:48, 78.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.02806713432073593 60\n",
      "    Loss: 0.025075459852814674 696\n",
      "    Loss: 0.026688462123274803 376\n",
      "    Loss: 0.025178654119372368 427\n",
      "    Loss: 0.02354380488395691 725\n",
      "    Loss: 0.027005383744835854 1449\n",
      "    Loss: 0.02584058605134487 1036\n",
      "    Loss: 0.02355206198990345 326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [15:38<10:20, 77.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.023981742560863495 486\n",
      "    Loss: 0.024443626403808594 1400\n",
      "    Loss: 0.022984229028224945 94\n",
      "    Loss: 0.02535443939268589 892\n",
      "    Loss: 0.02597607858479023 653\n",
      "    Loss: 0.02221301943063736 767\n",
      "    Loss: 0.02350631356239319 1144\n",
      "    Loss: 0.021951740607619286 1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [16:58<09:09, 78.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.021773425862193108 1420\n",
      "    Loss: 0.022963574156165123 1175\n",
      "    Loss: 0.02383040077984333 1351\n",
      "    Loss: 0.021677039563655853 1261\n",
      "    Loss: 0.02522192895412445 857\n",
      "    Loss: 0.02214350365102291 583\n",
      "    Loss: 0.020491937175393105 480\n",
      "    Loss: 0.02355988137423992 1326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [18:15<07:46, 77.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.023366563022136688 456\n",
      "    Loss: 0.020729107782244682 1021\n",
      "    Loss: 0.021366924047470093 1095\n",
      "    Loss: 0.022078434005379677 202\n",
      "    Loss: 0.023478815332055092 419\n",
      "    Loss: 0.02134416252374649 420\n",
      "    Loss: 0.022082896903157234 83\n",
      "    Loss: 0.02119002863764763 1256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [19:37<06:36, 79.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.02158377878367901 107\n",
      "    Loss: 0.0221440177410841 612\n",
      "    Loss: 0.021294469013810158 227\n",
      "    Loss: 0.023193947970867157 688\n",
      "    Loss: 0.02094435691833496 840\n",
      "    Loss: 0.02127321995794773 511\n",
      "    Loss: 0.021403148770332336 769\n",
      "    Loss: 0.0212516151368618 1465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [20:54<05:14, 78.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.01928972266614437 37\n",
      "    Loss: 0.02184154838323593 1222\n",
      "    Loss: 0.020813291892409325 1220\n",
      "    Loss: 0.022160368040204048 211\n",
      "    Loss: 0.024582235142588615 123\n",
      "    Loss: 0.02205674909055233 188\n",
      "    Loss: 0.019367888569831848 250\n",
      "    Loss: 0.021928558126091957 1264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [22:12<03:55, 78.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.020331883803009987 919\n",
      "    Loss: 0.021874485537409782 834\n",
      "    Loss: 0.019039006903767586 284\n",
      "    Loss: 0.020743979141116142 1268\n",
      "    Loss: 0.02180296741425991 1161\n",
      "    Loss: 0.020646555349230766 897\n",
      "    Loss: 0.02406133897602558 1082\n",
      "    Loss: 0.023127567023038864 1371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [23:30<02:36, 78.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.019980700686573982 1355\n",
      "    Loss: 0.020837604999542236 420\n",
      "    Loss: 0.021363260224461555 811\n",
      "    Loss: 0.020654087886214256 1396\n",
      "    Loss: 0.022064028307795525 272\n",
      "    Loss: 0.022814972326159477 1417\n",
      "    Loss: 0.022364402189850807 186\n",
      "    Loss: 0.02093079313635826 925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [24:55<01:20, 80.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Loss: 0.021123258396983147 787\n",
      "    Loss: 0.02229459397494793 637\n",
      "    Loss: 0.02010589838027954 1168\n",
      "    Loss: 0.02162044681608677 1105\n",
      "    Loss: 0.02133283019065857 1468\n",
      "    Loss: 0.02094140462577343 222\n",
      "    Loss: 0.02171563357114792 564\n",
      "    Loss: 0.022077005356550217 452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [26:14<00:00, 78.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard用のログディレクトリを指定\n",
    "writer = SummaryWriter(log_dir=\"/mnt/logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "epoch = 0\n",
    "\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for j in range(num_train_mesh):\n",
    "        save_mesh(train_mesh_data_list[j], f\"{epoch}_{j}\")\n",
    "\n",
    "for epoch in tqdm(range(num_train_epoch)):\n",
    "    #print(\"epoch:\", epoch)\n",
    "    train(device)\n",
    "\n",
    "    # for i in range(num_train_mesh):\n",
    "    #     plot_mesh(train_mesh_data_list[i], f\"{i}\")\n",
    "    for j in range(num_train_mesh):\n",
    "        save_mesh(train_mesh_data_list[j], f\"{epoch+1}_{j}\")\n",
    "    \n",
    "writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = data_getter(26, 0, train_mesh_data_list, train_polygon_data_list)\n",
    "polygon.faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon.coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_ = normalization(polygon)\n",
    "polygon_.coordinates +torch.tensor([-0.08619694411754608, 0.08077329397201538])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(polygon.coordinates.T[0], polygon.coordinates.T[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_train_mesh):\n",
    "    plot_mesh(train_mesh_data_list[i], f\"{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(device, trial, test_mesh_data_lists):\n",
    "    model.eval()\n",
    "    for step, data in enumerate(test_data_loader):\n",
    "        empty_cache()\n",
    "        gc.collect()\n",
    "        # print(f\"Step {step + 1}:\")\n",
    "        # print(\"==========\")\n",
    "        # print(data)\n",
    "        # print(len(data))\n",
    "        minibatch = Minibatch()\n",
    "        minibatch_coordinates = []\n",
    "        all_edge_index_1 = []\n",
    "        all_edge_index_2 = []\n",
    "        batch_list = []\n",
    "        num_dis = 0\n",
    "        metric_loss_list = []\n",
    "        for i in range(len(data)):\n",
    "            # gc.collect()\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            # print(\"polygonID:\",polygonID)\n",
    "            polygon = data_getter(polygonID, trial, test_mesh_data_list, test_polygon_data_list)\n",
    "            # polygon を正規化する\n",
    "            polygon = normalization(polygon)\n",
    "            # print(\"polygon.coordinates:\",polygon.coordinates)\n",
    "            edge_index = polygon.edge_index + num_dis\n",
    "            # print(\"polygon.edge_index:\", edge_index)\n",
    "            all_edge_index_1.append(edge_index[0])\n",
    "            all_edge_index_2.append(edge_index[1])\n",
    "            num_dis = num_dis + len(polygon.coordinates)\n",
    "            # print(\"==========\")\n",
    "            minibatch_coordinates.append(polygon.coordinates)\n",
    "            batch_i = torch.tensor([i]*len(polygon.coordinates))\n",
    "            batch_list.append(batch_i)\n",
    "            \n",
    "        # print(minibatch_coordinates)\n",
    "        minibatch.x = torch.cat(minibatch_coordinates, dim=0)\n",
    "        \n",
    "        # print(minibatch.x)\n",
    "        # print(\"minibatch.x.size:\", minibatch.x.size())\n",
    "        edge_index_1 = torch.cat(all_edge_index_1, dim=-1)\n",
    "        edge_index_2 = torch.cat(all_edge_index_2, dim=-1)\n",
    "        minibatch.edge_index = torch.cat([edge_index_1.unsqueeze(0), edge_index_2.unsqueeze(0)], dim=0)\n",
    "        # print(minibatch.edge_index)\n",
    "        # print(\"minibatch.edge_index.size:\", minibatch.edge_index.size())\n",
    "        minibatch.batch = torch.cat(batch_list, dim=0)\n",
    "        # print(minibatch.batch)\n",
    "        # print(\"minibatch.batch.size:\", minibatch.batch.size())\n",
    "\n",
    "        # すべてのデータをGPU上に移動する\n",
    "        minibatch.x = minibatch.x.to(device)\n",
    "        minibatch.edge_index = minibatch.edge_index.to(device)\n",
    "        minibatch.batch = minibatch.batch.to(device)\n",
    "        model.to(device)\n",
    "\n",
    "        out = model(minibatch.x, minibatch.edge_index, minibatch.batch)\n",
    "        # print(\"out:\", out)\n",
    "        # print(\"out:\", out.size())\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            # gc.collect()\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            polygon = data_getter(polygonID, trial, test_mesh_data_list, test_polygon_data_list)\n",
    "            # 正規化する\n",
    "            polygon = normalization(polygon)\n",
    "\n",
    "            polygon.coordinates[0] = out[i]\n",
    "            \n",
    "            # print(\"out_i:\", out[i])\n",
    "            polygon = check(polygon, polygonID)\n",
    "            # print(\"fixed_out_i:\", polygon.coordinates[0])\n",
    "            # metric_loss = criterion(polygon)\n",
    "            # metric_loss_list.append(metric_loss)\n",
    "            \n",
    "            # 非正規化する\n",
    "            polygon = denormalization(polygon)\n",
    "\n",
    "            # 予測したノードの座標をもとのメッシュに当てはめて更新する\n",
    "            polygon_meshID = int(test_polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "            mesh = test_mesh_data_lists[trial][polygon_meshID]\n",
    "\n",
    "            mesh.coordinates[test_polygon_data_list[polygonID].nodeID[0]] = polygon.coordinates[0]\n",
    "        \n",
    "    return test_mesh_data_lists\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100epochで最適化されたメッシュを10個生成する\n",
    "test_mesh_data_lists = [copy.deepcopy(test_mesh_data_list) for _ in range(10)]\n",
    "\n",
    "for trial in range(num_trial):\n",
    "    for epoch in tqdm(range(num_test_epoch)):\n",
    "        test_mesh_data_lists = test(device, trial, test_mesh_data_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mesh_data_list = []\n",
    "num_test_mesh = len(test_vtk_files)\n",
    "    \n",
    "for i in tqdm(range(num_test_mesh)):\n",
    "    q_hat_list = []\n",
    "    for j in range(10):\n",
    "        mesh = test_mesh_data_lists[j][i]\n",
    "\n",
    "        # q_hat を求めるコード\n",
    "        q_hat = calculate_q_hat(mesh)\n",
    "        \n",
    "        q_hat_list.append(q_hat)\n",
    "        best = q_hat_list.index(min(q_hat_list))\n",
    "        best_mesh_data_list.append(test_mesh_data_lists[best + 1][i])\n",
    "    \n",
    "\n",
    "\n",
    "    # best_mesh_data_list[i] のデータをvtkファイルで出力する\n",
    "    vtk_output(best_mesh_data_list[i], f\"optimized_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新したメッシュを表示してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_test_mesh):\n",
    "    plot_mesh(best_mesh_data_list[i], f\"optimized_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 元のメッシュから座標が変わっているか確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "格子点が滑らかに配置されているか、急激な変化や不連続がないかを評価します。# for i in range(num_test_mesh):\n",
    "#     print(test_mesh_data_list[i].coordinates.size())\n",
    "#     print(best_mesh_data_list[i].coordinates.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_test_mesh):\n",
    "    if torch.equal(test_mesh_data_list[i].coordinates, best_mesh_data_list[i].coordinates):\n",
    "        print(\"Not updated!!\")\n",
    "    \n",
    "    else:\n",
    "         print(\"Updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_test_mesh):\n",
    "    plot_mesh(test_mesh_data_list[i], f\"original_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.tensor([[[ 0.6004, -0.1381,  0.5997,  0.2204, -0.5977,  0.4884, -0.1954, -0.2755],\n",
    "        [-0.5785, -0.7031,  0.2004, -0.1510,  0.2774, -0.5800,  0.5260, -0.5150],\n",
    "        [-0.1189,  0.1473,  0.3666,  0.5725,  0.6517, -0.5593,  0.1796, -0.3000],\n",
    "        [-0.0748, -0.5303,  0.6381, -0.5197,  0.3724,  0.2501,  0.2280, -0.3767]],\n",
    "        [[0.6353,  0.1526,  0.1002, -0.6209,  0.2963, -0.1024, -0.3281,  0.6075],\n",
    "        [0.1588, -0.3860, -0.3635, -0.0385,  0.3957, -0.1853, -0.4013, -0.2480],\n",
    "        [-0.5284,  0.2546,  0.5519, -0.6643,  0.1631,  0.3607,  0.1233, -0.2522],\n",
    "        [0.3676,  0.3681,  0.2616, -0.1182, -0.1916,  0.0747, -0.1194, -0.2073]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 3点の座標を定義（これらは微分の対象となる変数なのでrequires_grad=Trueを設定）\n",
    "p1 = torch.tensor([-0.6721, -0.4008], requires_grad=True)\n",
    "p2 = torch.tensor([-0.0425,  0.0179], requires_grad=True)\n",
    "p3 = torch.tensor([-0.3361, -0.1763], requires_grad=True)\n",
    "\n",
    "# 各辺の長さを計算\n",
    "a = torch.sqrt(torch.sum((p2 - p1) ** 2))\n",
    "b = torch.sqrt(torch.sum((p3 - p2) ** 2))\n",
    "c = torch.sqrt(torch.sum((p1 - p3) ** 2))\n",
    "\n",
    "# ヘロンの公式を用いて面積を計算\n",
    "s = (a + b + c) / 2\n",
    "area = torch.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "\n",
    "loss = 1 - (4*torch.sqrt(torch.tensor(3))*area)/(a**2 + b**2 + c**2)\n",
    "\n",
    "# 面積に対して自動微分を行う\n",
    "loss.backward()\n",
    "\n",
    "# 各点に対する勾配を表示\n",
    "print(p1.grad)\n",
    "print(p2.grad)\n",
    "print(p3.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(torch.tensor(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[0.0, 0.0], \n",
    "    [-0.3022986352443695, 0.7041022777557373],\n",
    "    [-0.6732527613639832, 0.49798783659935],\n",
    "    [-0.6732527613639832, -0.05284641683101654],\n",
    "    [-0.3600771427154541, -0.19979676604270935],\n",
    "    [0.1442829966545105, -0.10576343536376953],\n",
    "    [0.32674723863601685, 0.25243768095970154]]\n",
    "\n",
    "for i in range(1, len(a)):\n",
    "    x = [a[0][0], a[i][0]]\n",
    "    y = [a[0][1], a[i][1]]\n",
    "    plt.plot(x, y, c=\"k\")\n",
    "    \n",
    "for i in range(1, len(a)):\n",
    "    x = [a[i][0], a[i%6+1][0]]\n",
    "    y = [a[i][1], a[i%6+1][1]]\n",
    "    plt.plot(x, y, c=\"k\")\n",
    "    \n",
    "plt.scatter([x[0] for x in a], [x[1] for x in a], c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[-0.08619694411754608, 0.08077329397201538], \n",
    "    [-0.3022986352443695, 0.7041022777557373],\n",
    "    [-0.6732527613639832, 0.49798783659935],\n",
    "    [-0.6732527613639832, -0.05284641683101654],\n",
    "    [-0.3600771427154541, -0.19979676604270935],\n",
    "    [0.1442829966545105, -0.10576343536376953],\n",
    "    [0.32674723863601685, 0.25243768095970154]]\n",
    "\n",
    "for i in range(1, len(a)):\n",
    "    x = [a[0][0], a[i][0]]\n",
    "    y = [a[0][1], a[i][1]]\n",
    "    plt.plot(x, y, c=\"k\")\n",
    "    \n",
    "for i in range(1, len(a)):\n",
    "    x = [a[i][0], a[i%6+1][0]]\n",
    "    y = [a[i][1], a[i%6+1][1]]\n",
    "    plt.plot(x, y, c=\"k\")\n",
    "    \n",
    "plt.scatter([x[0] for x in a], [x[1] for x in a], c=\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
