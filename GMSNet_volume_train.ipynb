{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train copy.ipynb を改変し、GMSNetを三次元に拡張したい\n",
    "\n",
    "各種の関数は２次元用に作られているため、３次元用に改変する必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シフト切り捨てをやめてみる\n",
    "# トレーニング完了後のトレーニングメッシュを表示してみる\n",
    "# 各epochの終了時にメッシュを可視化してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GraphNorm, LayerNorm\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch.nn import Linear, InstanceNorm2d, InstanceNorm1d, Conv1d, ReLU, Tanh\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.transforms import FaceToEdge\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from itertools import combinations\n",
    "import vtk\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "# 計算を軽くするためのライブラリ\n",
    "from torch.cuda import empty_cache\n",
    "import gc               # メモリリークを防ぐ\n",
    "\n",
    "from torch import nn\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import datetime\n",
    "import pyvista as pv\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epoch = 30\n",
    "train_data_path = \"/mnt/volume_data_folder/\"\n",
    "save_fig_path = \"/mnt/volume_training/\"\n",
    "vtk_save_path = \"/mnt/vtk_output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, num_files):\n",
    "        None\n",
    "\n",
    "class Mesh(Dataset):\n",
    "    def __init__(self):\n",
    "        self.coordinates = None\n",
    "        self.cells = None\n",
    "        self.faces = None\n",
    "\n",
    "class Polygon(Dataset):\n",
    "    def __init__(self, num_node, num_cells):\n",
    "        self.parent_meshID = None\n",
    "        self.coordinates = torch.zeros(num_node, 3)\n",
    "        self.cells = torch.zeros(num_cells, 4)\n",
    "        self.edge_index = None\n",
    "        self.edges = None\n",
    "        self.d = None\n",
    "        self.Cx = None\n",
    "        self.Cy = None\n",
    "        self.Cz = None\n",
    "        self.x_min = None\n",
    "        self.y_min = None\n",
    "        self.z_min = None\n",
    "\n",
    "    def to(self, device):\n",
    "        # GPUに移動可能なtorch.Tensor変数を移動\n",
    "        self.coordinates = self.coordinates.to(device)\n",
    "        self.cells = self.cells.to(device)\n",
    "        if self.edge_index is not None:\n",
    "            self.edge_index = self.edge_index.to(device)\n",
    "        if self.edges is not None:\n",
    "            self.edges = self.edges.to(device)\n",
    "        if self.d is not None:\n",
    "            self.d = self.d.to(device)\n",
    "        if self.Cx is not None:\n",
    "            self.Cx = self.Cx.to(device)\n",
    "        if self.Cy is not None:\n",
    "            self.Cy = self.Cy.to(device)\n",
    "        if self.Cz is not None:\n",
    "            self.Cz = self.Cz.to(device)\n",
    "        if self.x_min is not None:\n",
    "            self.x_min = self.x_min.to(device)\n",
    "        if self.y_min is not None:\n",
    "            self.y_min = self.y_min.to(device)\n",
    "        if self.z_min is not None:\n",
    "            self.z_min = self.z_min.to(device)\n",
    "\n",
    "class PolygonID(Dataset):\n",
    "    def __init__(self, nodeID):\n",
    "        self.nodeID = nodeID\n",
    "        # self.parent_meshID = None\n",
    "\n",
    "class Polygon_data(Dataset):\n",
    "    def __init__(self, polygonID, meshID, nodeID, num_cells):\n",
    "        self.polygonID = polygonID\n",
    "        self.meshID = meshID\n",
    "        self.nodeID = nodeID\n",
    "        self.num_cells = num_cells\n",
    "\n",
    "class Minibatch(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.edge_index = None\n",
    "        self.batch = None\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience      # 損失が改善しないエポック数の上限\n",
    "        self.min_delta = min_delta    # 損失が改善とみなす最小値\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh_polygonID_data(vtk_file_path, polygonID_list, poly_count, polygon_dict, mesh_index):\n",
    "    data = pv.read(vtk_file_path)\n",
    "    mesh = Mesh()\n",
    "    mesh.coordinates = torch.tensor(data.points, dtype=torch.float32)\n",
    "    cells = data.cells\n",
    "    celltypes = data.celltypes\n",
    "    tetra_indices = torch.where(torch.tensor(celltypes) == 10)[0]\n",
    "    tetra_cells = []\n",
    "    face_cells = []\n",
    "    cell_offset = 0\n",
    "\n",
    "    for idx in tetra_indices:\n",
    "        num_points = cells[cell_offset]\n",
    "        if num_points == 4:\n",
    "            tetra_cells.append(cells[cell_offset + 1 : cell_offset + 1 + num_points])\n",
    "        elif num_points == 3:\n",
    "            face_cells.append(cells[cell_offset + 1 : cell_offset + 1 + num_points])\n",
    "            \n",
    "        cell_offset += num_points + 1\n",
    "\n",
    "    mesh.cells = torch.tensor(tetra_cells, dtype=torch.long)\n",
    "    # print(\"cells:\", mesh.cells)\n",
    "\n",
    "    mesh.faces = torch.tensor(face_cells, dtype=torch.long)\n",
    "# ------------ mesh のデータを取得完了 -------------------------\n",
    "\n",
    "# ---------------- 自由点かどうかの判定完了 ------------------------\n",
    "    \n",
    "    boundary_faces = data.extract_feature_edges(boundary_edges=True, manifold_edges=False)  #境界面の取得\n",
    "    boundary_points_indices = boundary_faces.point_data[\"vtkOriginalPointIds\"]              #境界面に存在する頂点番号の取得\n",
    "    boundary_points = set(boundary_points_indices)                                        #境界面の頂点番号をリストに変換\n",
    "    \n",
    "    count= 0\n",
    "    for pointId in range(mesh.coordinates.size(0)):       # pointId:自由点の頂点番号\n",
    "        if pointId in boundary_points:\n",
    "            continue\n",
    "        else:\n",
    "            poly_count = poly_count + 1\n",
    "            # print(\"pointId:\", pointId)\n",
    "        mask = (mesh.cells == pointId)\n",
    "        if mask.any():\n",
    "            count = torch.sum(mask).item()\n",
    "        num_node = count + 1\n",
    "        num_tetra = count\n",
    "        polygon_number = poly_count - 1 \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        polygon_i = f\"polygon_{polygon_number}\"\n",
    "        # print(polygon_i)\n",
    "        polygon_i = Polygon(num_node, num_tetra)\n",
    "        \n",
    "        element_to_check = pointId\n",
    "        polygon_i.cells = mesh.cells[(mesh.cells == element_to_check).any(dim=1)]\n",
    "        # print(polygon_i.face)\n",
    "\n",
    "        polygon_i.nodeId = set()\n",
    "        for i in range(len(polygon_i.cells)):\n",
    "            polygon_i.nodeId.add(polygon_i.cells[i, 0].item())\n",
    "            polygon_i.nodeId.add(polygon_i.cells[i, 1].item())\n",
    "            polygon_i.nodeId.add(polygon_i.cells[i, 2].item())\n",
    "            polygon_i.nodeId.add(polygon_i.cells[i, 3].item())\n",
    "        sorted_nodeId = sorted(polygon_i.nodeId)\n",
    "        polygon_i.nodeID = torch.tensor(list(sorted_nodeId))\n",
    "        \n",
    "        point_id_index = (polygon_i.nodeID == pointId).nonzero().item()\n",
    "\n",
    "        value_to_move = polygon_i.nodeID[point_id_index]\n",
    "        polygon_i.nodeID = torch.cat((value_to_move.unsqueeze(0), polygon_i.nodeID[polygon_i.nodeID != pointId]))\n",
    "        # print(polygon_i.nodeID)\n",
    "        setattr(polygon_i, \"parent_meshID\", mesh)\n",
    "        polygonID_list.append(f\"polygon_{polygon_number}\")\n",
    "\n",
    "        keyword = f\"polygon_{polygon_number}\"\n",
    "        valiables = (f\"mesh_{mesh_index}\", polygon_i.nodeID, len(polygon_i.cells))\n",
    "        polygon_dict[keyword] = valiables\n",
    "\n",
    "\n",
    "    # --------- polygon.nodeID の取得完了 -------------\n",
    "    return mesh, polygonID_list, poly_count, polygon_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mesh_polygon_dataset(vtk_files):\n",
    "    num_vtk_files = len(vtk_files)\n",
    "    polygonID_list = []\n",
    "    mesh_data_list = []\n",
    "    poly_count = 0\n",
    "    polygon_dict = {}\n",
    "    # ファイルに順にアクセスする\n",
    "    for i in range(num_vtk_files):\n",
    "        # print(\"File Name:\", vtk_files[i])\n",
    "        mesh, polygonID_list, poly_count, polygon_dict = create_mesh_polygonID_data(vtk_files[i], polygonID_list, poly_count, polygon_dict, i)\n",
    "        mesh_data_list.append(mesh)\n",
    "    return mesh_data_list, polygonID_list, polygon_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下、i はpolygon番号で座標とセル情報を取得することができる\n",
    "def data_getter(polygonID, num_mesh_data_list, mesh_data_list, polygon_data_list):\n",
    "    \n",
    "    polygon_meshID = int(polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "    mesh = mesh_data_list[polygon_meshID]\n",
    "    \n",
    "    num_node = len(polygon_data_list[polygonID].nodeID)\n",
    "    num_tetra = polygon_data_list[polygonID].num_cells\n",
    "    polygon_i = Polygon(num_node, num_tetra)\n",
    "\n",
    "    polygon_i.coordinates = mesh.coordinates[polygon_data_list[polygonID].nodeID]     # polygonの座標\n",
    "\n",
    "    \n",
    "    element_to_check = polygon_data_list[polygonID].nodeID[0]\n",
    "    polygon_i.tetra = mesh.cells[(mesh.cells == element_to_check).any(dim=1)]\n",
    "\n",
    "    indices = torch.nonzero(torch.isin(polygon_i.tetra, polygon_data_list[polygonID].nodeID))\n",
    "    for idx in range(indices.size(0)):\n",
    "        row_idx, col_idx = indices[idx]\n",
    "        value_to_replace = polygon_i.tetra[row_idx, col_idx]\n",
    "        polygon_i.tetra[row_idx, col_idx] = (polygon_data_list[polygonID].nodeID == value_to_replace).nonzero().item()\n",
    "    polygon_i.cells = polygon_i.tetra.long()\n",
    "\n",
    "    # 各行の三角形からエッジを抽出してedge_indexを構築\n",
    "    edges = torch.cat([ polygon_i.cells[:, [0, 1]],\n",
    "                        polygon_i.cells[:, [0, 2]],\n",
    "                        polygon_i.cells[:, [0, 3]],\n",
    "                        polygon_i.cells[:, [1, 2]],\n",
    "                        polygon_i.cells[:, [1, 3]],\n",
    "                        polygon_i.cells[:, [2, 3]]], dim=0)\n",
    "\n",
    "    # エッジのインデックスをソートして重複を削除\n",
    "    edge_index = torch.sort(edges, dim=1).values\n",
    "    edge_index = torch.tensor(sorted(edge_index.numpy().tolist())).unique(dim=0)\n",
    "    polygon_i.edge_index = torch.transpose(edge_index, 0, 1)\n",
    "    # print(polygon_i.edge_index)\n",
    "    return polygon_i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# メッシュをプロットする関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mesh(mesh, title):\n",
    "    \n",
    "    coordinates = mesh.coordinates.clone().detach()\n",
    "    edge_index = mesh.edge_index.clone().detach()\n",
    "\n",
    "    # 3Dプロットの設定\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # 頂点をプロット\n",
    "    ax.scatter(coordinates[:, 0], coordinates[:, 1], coordinates[:, 2], c='r', label='Vertices')\n",
    "\n",
    "    # エッジをプロット\n",
    "    for i in range(edge_index.shape[1]):  # 各エッジをループ\n",
    "        start_idx = edge_index[0, i]\n",
    "        end_idx = edge_index[1, i]\n",
    "        start_point = coordinates[start_idx]\n",
    "        end_point = coordinates[end_idx]\n",
    "        ax.plot(\n",
    "            [start_point[0], end_point[0]],\n",
    "            [start_point[1], end_point[1]],\n",
    "            [start_point[2], end_point[2]],\n",
    "            c='b'\n",
    "        )\n",
    "\n",
    "    # 軸ラベルの設定\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "\n",
    "    # タイトルと凡例\n",
    "    ax.set_title('3D Mesh Visualization')\n",
    "    ax.legend()\n",
    "\n",
    "    # 表示\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mesh(mesh, title):\n",
    "\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.cells\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, aspect=\"equal\")\n",
    "\n",
    "    # 描画するメッシュの頂点をプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'bo')  # 頂点を青色の点でプロット\n",
    "    # ax.plot(vertices[:,0], vertices[:,1], 'k-')  # 辺を黒色の線でプロット\n",
    "\n",
    "    # 各三角形をプロット\n",
    "    for face in faces:\n",
    "        v0, v1, v2 = vertices[face]\n",
    "        v0_np = v0.detach().numpy()\n",
    "        v1_np = v1.detach().numpy()\n",
    "        v2_np = v2.detach().numpy()\n",
    "        ax.plot([v0_np[0], v1_np[0], v2_np[0], v0_np[0]], [v0_np[1], v1_np[1], v2_np[1], v0_np[1]], 'b-')  # 三角形を赤色の線でプロット\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.axhline(0, color=\"black\", linewidth=0.001)\n",
    "    ax.axvline(0, color=\"black\", linewidth=0.001)\n",
    "\n",
    "    # plt.xlim(-300, 150)\n",
    "    # plt.ylim(-200, 1400)\n",
    "    plt.savefig(f\"{save_fig_path}{title}.png\", format=\"png\")\n",
    "    # plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meshデータからvtkファイルを出力する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtk_output(mesh, title):\n",
    "    vertices = mesh.coordinates  # Assumed to be an Nx3 array\n",
    "    cells = mesh.cells  # Assumed to be an Mx3 array\n",
    "    faces = mesh.faces\n",
    "    num_vertices = len(vertices)\n",
    "    num_cells = len(cells)\n",
    "    num_faces = len(faces)\n",
    "\n",
    "    filename = f\"{vtk_save_path}{title}.vtk\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        # VTK Header\n",
    "        f.write(\"# vtk DataFile Version 2.0\\n\")\n",
    "        f.write(f\"{title}\\n\")\n",
    "        f.write(\"ASCII\\n\")\n",
    "        f.write(\"DATASET POLYDATA\\n\")\n",
    "\n",
    "        # Write vertices\n",
    "        f.write(f\"POINTS {num_vertices} float\\n\")\n",
    "        for vertex in vertices:\n",
    "            f.write(f\"{vertex[0]} {vertex[1]} {vertex[2]}\\n\")\n",
    "\n",
    "        # Write cells (polygons)\n",
    "        f.write(f\"\\nPOLYGONS {num_cells} {num_cells * 5}\\n\")\n",
    "        for cell in cells:\n",
    "            f.write(f\"4 {cell[0]} {cell[1]} {cell[2]} {cell[3]}\\n\")\n",
    "\n",
    "        f.write(f\"\\nPOLYGONS {num_faces} {num_faces * 4}\\n\")\n",
    "        for face in faces:\n",
    "            f.write(f\"3 {face[0]} {face[1]} {face[2]}\\n\")\n",
    "\n",
    "        # Write cell types\n",
    "        f.write(f\"\\nCELL_TYPES {num_cells + num_faces}\\n\")\n",
    "        for _ in range(num_cells):\n",
    "            f.write(\"10\\n\")  \n",
    "        for _ in range(num_faces):\n",
    "            f.write(\"5\\n\")\n",
    "\n",
    "    print(f\"VTK file saved as {filename}\")\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(polygon):\n",
    "    vertices = polygon.coordinates\n",
    "    normalized_vertices = vertices.clone()\n",
    "    centered_vertices = vertices.clone()\n",
    "    # print(vertices)\n",
    "\n",
    "    max_x = torch.max(vertices[:,0])\n",
    "    min_x = torch.min(vertices[:,0])\n",
    "    max_y = torch.max(vertices[:,1])\n",
    "    min_y = torch.min(vertices[:,1])\n",
    "    max_z = torch.max(vertices[:,2])\n",
    "    min_z = torch.min(vertices[:,2])\n",
    "\n",
    "    diffs = vertices.unsqueeze(1) - vertices.unsqueeze(0)\n",
    "    length = torch.norm(diffs, dim=2)\n",
    "    polygon.d = torch.max(length)\n",
    "    polygon.x_min = min_x\n",
    "    polygon.y_min = min_y\n",
    "    polygon.z_min = min_z\n",
    "\n",
    "    normalized_vertices = (vertices - torch.tensor([polygon.x_min, polygon.y_min, polygon.z_min])) / polygon.d\n",
    "\n",
    "    \n",
    "    polygon.Cx = torch.tensor(normalized_vertices[0,0].item())\n",
    "    polygon.Cy = torch.tensor(normalized_vertices[0,1].item())\n",
    "    polygon.Cz = torch.tensor(normalized_vertices[0,2].item())\n",
    "\n",
    "    centered_vertices = normalized_vertices - torch.tensor([polygon.Cx, polygon.Cy, polygon.Cz])\n",
    "    polygon.coordinates = centered_vertices\n",
    "    \n",
    "    # print(\"Normalized polygon:\", vertices)\n",
    "\n",
    "    return polygon\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalization(polygon):\n",
    "    vertices = polygon.coordinates.clone().to(device)\n",
    "    shifted_vertices = vertices.clone().to(device)\n",
    "    denormalized_vertices = vertices.clone().to(device)\n",
    "    \n",
    "    shifted_vertices = vertices + torch.tensor([polygon.Cx, polygon.Cy, polygon.Cz]).to(device)\n",
    "        \n",
    "\n",
    "    denormalized_vertices = polygon.d * shifted_vertices + torch.tensor([polygon.x_min, polygon.y_min, polygon.z_min]).to(device)\n",
    "    polygon.coordinates = denormalized_vertices\n",
    "    return polygon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ログをファイルに保存するようにしている\n",
    "import sys\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename='metric_loss.log',\n",
    "    level=logging.DEBUG, \n",
    "    format='%(message)s'\n",
    ")\n",
    "\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "class MetricLoss:\n",
    "    def select_vertices(self, vertices, cell):\n",
    "        v0 = vertices[cell[0]].clone().to(device)\n",
    "        v1 = vertices[cell[1]].clone().to(device)\n",
    "        v2 = vertices[cell[2]].clone().to(device)\n",
    "        v3 = vertices[cell[3]].clone().to(device)\n",
    "        if torch.isnan(v0).any() or torch.isnan(v1).any() or torch.isnan(v2).any() or torch.isnan(v3).any():\n",
    "            print(\"Error in vertices\")\n",
    "            \n",
    "        return v0, v1, v2, v3\n",
    "\n",
    "    def edge_length(self, v0, v1, v2, v3):\n",
    "        l1 = torch.sqrt(torch.sum((v0 - v1)**2))\n",
    "        l2 = torch.sqrt(torch.sum((v0 - v2)**2))\n",
    "        l3 = torch.sqrt(torch.sum((v0 - v3)**2))\n",
    "        l4 = torch.sqrt(torch.sum((v1 - v2)**2))\n",
    "        l5 = torch.sqrt(torch.sum((v1 - v3)**2))\n",
    "        l6 = torch.sqrt(torch.sum((v2 - v3)**2))\n",
    "        if torch.isnan(l1) or torch.isnan(l2) or torch.isnan(l3):\n",
    "            print(\"Error in edge_length\")\n",
    "        return l1, l2, l3, l4, l5, l6\n",
    "\n",
    "    def cell_volume(self, polygon, v0, v1, v2, v3):\n",
    "        \n",
    "        M = torch.tensor([\n",
    "                [v1[0]-v0[0], v1[1]-v0[1], v1[2]-v0[2]],\n",
    "                [v2[0]-v0[0], v2[1]-v0[1], v2[2]-v0[2]],\n",
    "                [v3[0]-v0[0], v3[1]-v0[1], v3[2]-v0[2]]\n",
    "            ], dtype=torch.float64)\n",
    "\n",
    "        # 行列式を計算\n",
    "        det_M = torch.det(M)\n",
    "\n",
    "        # 体積を計算\n",
    "        volume = torch.tensor(torch.abs(det_M)/ 6)\n",
    "        return volume.item()\n",
    "\n",
    "    def compute_loss(self, polygon, vertices, cell, dx):\n",
    "        v0, v1, v2, v3 = self.select_vertices(vertices, cell)\n",
    "\n",
    "        l1, l2, l3, l4, l5, l6 = self.edge_length(v0, v1, v2, v3)\n",
    "        length_tensor = torch.tensor([l1, l2, l3, l4, l5, l6], dtype=torch.float32)\n",
    "        v = self.cell_volume(polygon,  v0, v1, v2, v3)\n",
    "        loss = 1-((36.0*torch.sqrt(torch.tensor(2.))*v)/(torch.sum(length_tensor ** 3)))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def __call__(self, polygon, dx=None):\n",
    "        vertices = polygon.coordinates\n",
    "        cells = polygon.cells\n",
    "        loss = torch.tensor(0.0)\n",
    "        for cell in cells:\n",
    "            loss = loss + self.compute_loss(polygon, vertices, cell, dx)\n",
    "        \n",
    "        metric_loss = loss/len(cells)\n",
    "        if torch.isnan(metric_loss):\n",
    "            print(\"coordinates:\", vertices)\n",
    "            print(\"cells:\", cells)  \n",
    "            plot_mesh(polygon, \"Current Polygon\")\n",
    "            print(\"Error: Loss value is NaN. Exiting program.\")\n",
    "            sys.exit(1)\n",
    "        return metric_loss\n",
    "    \n",
    "    \n",
    "def print_grad(grad):\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# meshデータからq_hatを求める関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_q_hat(mesh):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.cells\n",
    "    r_list = []\n",
    "    alpha_list = []\n",
    "    beta_list = []\n",
    "\n",
    "    for face in faces:\n",
    "        # a(最小角)と b(最大角)を求める\n",
    "\n",
    "        angles = []\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "\n",
    "        # 余弦定理から各角度の余弦値を計算\n",
    "        cos_alpha = (l2**2 + l3**2 - l1**2) / (2*l2*l3)\n",
    "        cos_beta = (l1**2 + l3**2 - l2**2) / (2*l1*l3)\n",
    "        cos_gamma = (l1**2 + l2**2 - l3**2) / (2*l1*l2)\n",
    "        # 余弦値から角度を計算して個度法に変換\n",
    "        alpha = torch.acos(cos_alpha) * 180 / np.pi\n",
    "        beta = torch.acos(cos_beta) * 180 / np.pi\n",
    "        gamma = torch.acos(cos_gamma) * 180 / np.pi\n",
    "\n",
    "        angles.append(alpha)\n",
    "        angles.append(beta)\n",
    "        angles.append(gamma)\n",
    "\n",
    "        min_angle = min(angles)\n",
    "        max_angle = max(angles)\n",
    "\n",
    "        alpha_list.append(min_angle)\n",
    "        beta_list.append(max_angle)\n",
    "\n",
    "    # 1/q = r を求める\n",
    "    for i in range(len(test_polygonID_list)):\n",
    "        polygonID = i \n",
    "        polygon = data_getter(polygonID, 0, test_mesh_data_lists, test_polygon_data_list)\n",
    "\n",
    "        r = 1 - m_loss(polygon) \n",
    "        r_list.append(r)\n",
    "\n",
    "    a_mean = sum(alpha_list) / len(alpha_list)\n",
    "    a_min = min(alpha_list)\n",
    "    b_mean = sum(beta_list) / len(beta_list)\n",
    "    b_max = max(beta_list)\n",
    "    r_mean = sum(r_list) / len(r_list)\n",
    "    r_min = min(r_list)\n",
    "\n",
    "    q_hat = (((a_mean + a_min + 120 - b_max - b_mean)/60) + r_mean + r_min) / 6\n",
    "\n",
    "    return q_hat\n",
    "\n",
    "    \n",
    "\n",
    "def calculate_qhat(mesh):\n",
    "    vertices = mesh.coordinates\n",
    "    faces = mesh.cells\n",
    "    r_list = []\n",
    "    alpha_list = []\n",
    "    beta_list = []\n",
    "\n",
    "    for face in faces:\n",
    "        # a(最小角)と b(最大角)を求める\n",
    "\n",
    "        angles = []\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "\n",
    "        # 余弦定理から各角度の余弦値を計算\n",
    "        cos_alpha = (l2**2 + l3**2 - l1**2) / (2*l2*l3)\n",
    "        cos_beta = (l1**2 + l3**2 - l2**2) / (2*l1*l3)\n",
    "        cos_gamma = (l1**2 + l2**2 - l3**2) / (2*l1*l2)\n",
    "        # 余弦値から角度を計算して個度法に変換\n",
    "        alpha = torch.acos(cos_alpha) * 180 / np.pi\n",
    "        beta = torch.acos(cos_beta) * 180 / np.pi\n",
    "        gamma = torch.acos(cos_gamma) * 180 / np.pi\n",
    "\n",
    "        angles.append(alpha)\n",
    "        angles.append(beta)\n",
    "        angles.append(gamma)\n",
    "\n",
    "        min_angle = min(angles)\n",
    "        max_angle = max(angles)\n",
    "\n",
    "        alpha_list.append(min_angle)\n",
    "        beta_list.append(max_angle)\n",
    "\n",
    "        # 1/q = r を求める\n",
    "        v0, v1, v2 = m_loss.select_vertices(vertices, face)\n",
    "        l1, l2, l3 = m_loss.edge_length(v0, v1, v2)\n",
    "        s = 0.5*(l1 + l2 + l3)\n",
    "        temp = s*(s-l1)*(s-l2)*(s-l3)\n",
    "        loss = 1-(4.0*torch.sqrt(torch.tensor(3.))*s)/(l1**2 + l2**2 + l3**2)\n",
    "        r_list.append(1./loss)\n",
    "\n",
    "\n",
    "\n",
    "    a_mean = sum(alpha_list) / len(alpha_list)\n",
    "    a_min = min(alpha_list)\n",
    "    b_mean = sum(beta_list) / len(beta_list)\n",
    "    b_max = max(beta_list)\n",
    "    r_mean = sum(r_list) / len(r_list)\n",
    "    r_min = min(r_list)\n",
    "\n",
    "    q_hat = (((a_mean + a_min + 120 - b_max - b_mean)/60) + r_mean + r_min) / 6\n",
    "\n",
    "    return q_hat\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スターポリゴンの中から外側に自由点が移動したときに自由点の移動量を半分にしてもう一度外に行っていないか検証する\n",
    "自由点が外に行かないことを確認したあとのスターポリゴンを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(polygon):\n",
    "    # print(\"polygonID:\", polygonID)\n",
    "    vertices = polygon.coordinates\n",
    "    \n",
    "    edge_index = polygon.edge_index\n",
    "    \n",
    "    return_value = True\n",
    "    while return_value == True:   \n",
    "\n",
    "        for i in range(1, len(vertices[:,0])):\n",
    "            point1 = torch.tensor([0.0, 0.0])\n",
    "            point2 = vertices[0]\n",
    "            point3 = vertices[i]\n",
    "            \n",
    "            pos_i = torch.where(edge_index[0] == i)\n",
    "            pos_i = pos_i[0]\n",
    "            # print(\"edge_index[0]\", edge_index[0])\n",
    "            \n",
    "            for j in range(len(pos_i)):\n",
    "                if edge_index[1, pos_i[j]] == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    val_pos_i = edge_index[1, pos_i[j]]\n",
    "                    break\n",
    "\n",
    "            point4 = vertices[val_pos_i]\n",
    "\n",
    "\n",
    "            a1 = 0\n",
    "            b1 = 0\n",
    "            a2 = 0\n",
    "            b2 = 0\n",
    "            check1 = 0\n",
    "            check2 = 0\n",
    "            check3 = 0\n",
    "            check4 = 0\n",
    "            x1 = point1[0]\n",
    "            y1 = point1[1]\n",
    "            x2 = point2[0]\n",
    "            y2 = point2[1]\n",
    "            x3 = point3[0]\n",
    "            y3 = point3[1]\n",
    "            x4 = point4[0]\n",
    "            y4 = point4[1]\n",
    "            a1 = (y1 - y2)/(x1-x2)\n",
    "            b1 = y1 - (a1*x1)\n",
    "            a2 = (y3 - y4)/(x3-x4)\n",
    "            b2 = y3 - (a2*x3)\n",
    "            check1 = (a1*x3) - y3 + b1 \n",
    "            check2 = (a1*x4) - y4 + b1    # point1,2を通る直線に対してpoint3,4を結ぶ線分が交差しているか\n",
    "            check3 = (a2*x1) - y1 + b2\n",
    "            check4 = (a2*x2) - y2 + b2    # point3,4を通る直線に対してpoint1,2を結ぶ線分が交差しているか\n",
    "            # print(\"1:\",check1,\"2:\",check2,\"3:\",check3,\"4:\",check4)\n",
    "            del a1, a2, b1, b2, x1, x2, x3, x4, y1, y2, y3, y4 \n",
    "\n",
    "            if (check1*check2) <= 0.1 and (check3*check4) <= 0.1 :\n",
    "                return_value = True\n",
    "                # print(\"Out_of_StarPolygon\")\n",
    "                vertices[0] = 0.5*vertices[0]\n",
    "                polygon.coordinates[0] = vertices[0]\n",
    "                break\n",
    "            else:\n",
    "                return_value = False\n",
    "                continue       \n",
    "            \n",
    "        \n",
    "    # plot_mesh(polygon, \"polygon_checked\")\n",
    "               \n",
    "    return polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 隠れ層のノード数は何にするか未定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMSNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, feature_dim, hidden_channels):   # モデルの初期化\n",
    "        \n",
    "        super(GMSNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = Linear(input_dim, feature_dim)\n",
    "        self.gcn = GCNConv(feature_dim, feature_dim)\n",
    "        self.graph_norm = GraphNorm(feature_dim)\n",
    "        self.instance_norm = nn.InstanceNorm1d(feature_dim)\n",
    "        self.fc2 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.fc3 = nn.Linear(feature_dim, input_dim)\n",
    "\n",
    "\n",
    "        # Weight initialization\n",
    "        self.apply(self._init_weights)                                  # 重みの初期化\n",
    "\n",
    "    def _init_weights(self, m):     # 線形層と畳み込み層の重みをKaiming正規化で初期化し、バイアスをゼロで初期化する\n",
    "        if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x, edge_index):       \n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.graph_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn(x, edge_index) + x\n",
    "        x = self.fc2(x)\n",
    "        # x = self.instance_norm(x)\n",
    "        x = self.fc3(x)\n",
    "        x = 0.01 * x \n",
    "        x = x[0]\n",
    "\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_train_mesh: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_559775/2808576684.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  mesh.cells = torch.tensor(tetra_cells, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "# フォルダ内のすべてのvtkファイルにアクセスする\n",
    "train_vtk_files = glob.glob(f\"{train_data_path}*.vtk\")\n",
    "train_vtk_filenames = [file.split('/')[-1].split('.')[0] for file in train_vtk_files]\n",
    "\n",
    "num_train_mesh = len(train_vtk_files)\n",
    "print(\"num_train_mesh:\", num_train_mesh)\n",
    "train_mesh_data_list, train_polygonID_list, train_polygon_dict = create_mesh_polygon_dataset(train_vtk_files)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# ポリゴンデータを格納するリストを作成\n",
    "train_polygon_data_list = []\n",
    "\n",
    "for i in range(len(train_polygonID_list)):\n",
    "    polygonID = f\"polygon_{i}\"\n",
    "    meshID = train_polygon_dict[f\"polygon_{i}\"][0]\n",
    "    nodeID = train_polygon_dict[f\"polygon_{i}\"][1]\n",
    "    num_cells = train_polygon_dict[f\"polygon_{i}\"][2]\n",
    "    polygon_data = Polygon_data(polygonID, meshID, nodeID, num_cells)\n",
    "    train_polygon_data_list.append(polygon_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Polygon_data"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_polygon_data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_data_loader = DataLoader(train_polygonID_list, batch_size=64*num_train_mesh, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "GMSNet(\n",
      "  (fc1): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (gcn): GCNConv(64, 64)\n",
      "  (graph_norm): GraphNorm(64)\n",
      "  (instance_norm): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GMSNet(input_dim=3, feature_dim=64, hidden_channels=64)\n",
    "model.to(device)\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "criterion = MetricLoss()\n",
    "early_stopping = EarlyStopping(patience=5, min_delta=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer = SummaryWriter(\"logs\")\n",
    "# 学習率を調整するスケジューラの設定\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=2, factor=0.95, verbose=True)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "def train(device):\n",
    "    model.train()\n",
    "\n",
    "    # for name, param in model.GNorm.named_parameters():\n",
    "    #     print(f'{name}:{param.data}')\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    temp = 0\n",
    "    ddd = 0\n",
    "    \n",
    "    for step, data in enumerate(train_data_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        metric_loss = torch.tensor(0.0, requires_grad=True)\n",
    "        metric_loss.to(device)\n",
    "        dx_list = []\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "\n",
    "            polygonID = int(data[i].split(\"_\")[-1])\n",
    "            # print(\"polygonID:\", polygonID)\n",
    "            polygon = data_getter(polygonID, 0, train_mesh_data_list, train_polygon_data_list)\n",
    "            # plot_mesh(polygon, \"corrent_polygon\")\n",
    "            # vtk_output(polygon, \"corrent_polygon\")\n",
    "            # print(\"polygon_origin:\", polygon.coordinates)\n",
    "            polygon = normalization(polygon)           \n",
    "            \n",
    "            polygon.to(device)\n",
    "            # print(\"normalized polygon:\", polygon.coordinates)\n",
    "            x = polygon.coordinates.clone().to(device)\n",
    "            edge_index = polygon.edge_index.clone().to(device)\n",
    "            out = model(x, edge_index)\n",
    "            # print(\"out:\", out)\n",
    "            polygon.coordinates[0] = polygon.coordinates[0] + out\n",
    "            # polygon = check(polygon)\n",
    "            \n",
    "            \n",
    "            \n",
    "            # logger.info(\"epoch: {:04}, polygonID: {:04}\".format(epoch, polygonID))\n",
    "            # logger.info(\"before\")\n",
    "            with torch.no_grad():\n",
    "                l = criterion(polygon)\n",
    "                # print(\"metric_loss:\", l)\n",
    "\n",
    "            \n",
    "            # logger.info(\"\")\n",
    "            # print(l)\n",
    "            metric_loss = metric_loss + l\n",
    "            polygon = denormalization(polygon)\n",
    "            # print(\"changed polygon:\", polygon.coordinates)\n",
    "                \n",
    "            polygon_meshID = int(train_polygon_data_list[polygonID].meshID.split(\"_\")[-1])\n",
    "            mesh = train_mesh_data_list[polygon_meshID]\n",
    "            mesh.coordinates[train_polygon_data_list[polygonID].nodeID[0]] = polygon.coordinates[0]\n",
    "            \n",
    "            \n",
    "        loss = metric_loss/len(data)\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "        ddd = ddd + len(data)\n",
    "        temp = temp + loss\n",
    "        # print(\"    Loss:\", loss.item(), \"   PolygonID:\", polygonID)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss_ave = temp/ddd\n",
    "    writer.add_scalar(\"loss\", loss_ave, epoch)       \n",
    "    print(loss_ave, epoch)\n",
    "    loss_list.append(temp/ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(num_train_mesh):\n",
    "    # plot_mesh(train_mesh_data_list[i], f\"original_{train_vtk_filenames[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最終的な最適化したメッシュを生成してvtkファイルで出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TensorBoard用のログディレクトリを指定\n",
    "writer = SummaryWriter(log_dir=\"/mnt/log/test\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(num_train_epoch), position=1):\n",
    "#     #print(\"epoch:\", epoch)\n",
    "#     train(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_559775/2750639317.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  volume = torch.tensor(torch.abs(det_M)/ 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0028, grad_fn=<DivBackward0>) 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_1.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_2.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_3.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_4.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_5.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_6.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_7.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_8.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_9.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_10.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_11.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_12.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_13.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_14.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_15.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_16.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_17.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_18.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_19.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_20.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_21.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_22.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_23.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_24.vtk\n",
      "tensor(0.0028, grad_fn=<DivBackward0>) 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_25.vtk\n",
      "tensor(0.0029, grad_fn=<DivBackward0>) 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_26.vtk\n",
      "tensor(0.0029, grad_fn=<DivBackward0>) 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_27.vtk\n",
      "tensor(0.0029, grad_fn=<DivBackward0>) 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_28.vtk\n",
      "tensor(0.0029, grad_fn=<DivBackward0>) 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_29.vtk\n",
      "tensor(0.0029, grad_fn=<DivBackward0>) 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:40:04<00:00, 200.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VTK file saved as /mnt/vtk_output/trained_volume_30.vtk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# for j in range(num_train_mesh):                 # 元のメッシュを保存する\n",
    "#         save_mesh(train_mesh_data_list[j], f\"trained_{train_vtk_filenames[j]}_{epoch}\")\n",
    "# torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)            \n",
    "for epoch in tqdm(range(num_train_epoch), position=1):\n",
    "    #print(\"epoch:\", epoch)\n",
    "    train(device)\n",
    "    # print(epoch)\n",
    "\n",
    "    # for i in range(num_train_mesh):\n",
    "    #     plot_mesh(train_mesh_data_list[i], f\"{i}\")\n",
    "    for j in range(num_train_mesh):             # 最適化したメッシュを保存する\n",
    "        vtk_output(train_mesh_data_list[j], f\"trained_{train_vtk_filenames[j]}_{epoch+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMSNet(\n",
      "  (fc1): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (gcn): GCNConv(64, 64)\n",
      "  (graph_norm): GraphNorm(64)\n",
      "  (instance_norm): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "メモリの開放をする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
